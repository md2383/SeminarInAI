{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Practice/HW6 (10pt): Large Language Models and Their Implications\n",
        "<!-- ![img](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4470ce74-e595-4750-92a5-5f21f040df6d_577x432.jpeg) -->\n",
        "![img](https://i.imgur.com/QGYa2J8.jpeg)\n",
        "\n",
        "In this notebook, you're gonna play with some of the largest language models on the Internet.\n",
        "\n",
        "_Based on works of: Tim Dettmers, Ruslan Svirschevsky, Artem Chumachenko, Younes Belkada, Felix Marty, Yulian Gilyazev, Gosha Zolotov, Andrey Ishutin,  Elena Volf, Artemiy Vishnyakov, Svetlana Shirokovskih. [Source notebook](https://github.com/yandexdataschool/nlp_course/tree/2024/week06_llm)"
      ],
      "metadata": {
        "id": "aSWEcS2XKgzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: prompt engineering (4 points total)\n",
        "\n",
        "In the assignment, we'll use public APIs that host the 100B+ models for inference. Your task is to prompt-engineer the model into solving a few tasks for you.\n",
        "\n",
        "\n",
        "__Which API?__ You are free to use any publicly available API for general LM -- as long as it's __not a chat assistant__. So, gpt 3.5 is fine, but chatGPT is not. Here's a few options:\n",
        "\n",
        "- BLOOM API - [bigscience/bloom](https://huggingface.co/bigscience/bloom) (currently not supported)\n",
        "- OpenAI API - [openai.com/api](https://openai.com/api/)\n",
        "- Mixtral API - https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\n",
        "- AI21 Jurrasic API - [ai21.com](https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1)\n",
        "\n",
        "These APIs may require you to create a (free) account on their platform. Please note that some APIs also have paid subscriptions. __You do not need to pay them__, this assignment was designed to be solved using free-tier subscriptions. If no APIs work for you, you can also solve these tasks with the 6.7B model that you will find later in this notebook - but this will make the tasks somewhat harder.\n",
        "\n",
        "__Quests:__ you will need to solve 4 problems. For each one, please attach a short __description__ of your solution and the output from the API you use. _[If you use python APIs, show your python code with outputs]_\n",
        "\n",
        "__Example:__ Tony is talking to Darth Vader ([BLOOM API](https://huggingface.co/bigscience/bloom)). Black text is written manually, blue text is generated.\n",
        "<hr>\n",
        "\n",
        "![img](https://i.imgur.com/a1QhKF7.png)\n",
        "<hr>\n",
        "\n",
        "__It is fine to roll back a few times,__ e.g. in the example above, the model first generated Vader lines twice in a row, and we rolled that back. However, if you need more than 1-2 rollbacks per session, you should probably try a different prompt."
      ],
      "metadata": {
        "id": "1jYrxHF8Kgzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Task 1 (0.5 pt):__ arange a conversation between any two of the following:\n",
        "\n",
        "- a celebrity or politician of your choice\n",
        "- any fictional character (except Darth Vader)\n",
        "- yourself\n",
        "\n",
        "Compare two setups: a) you prompt with character names only b) you supply additional information (see example)."
      ],
      "metadata": {
        "id": "CHIvIFjsKgzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <your code OR writeup OR screenshots>"
      ],
      "metadata": {
        "id": "yJJtTnJEKgzm",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Please choose task 2a or 2b (0.5 pt)__ depending on your model (you can do both, but you will be awarded points for one of these two tasks).\n",
        "\n",
        "__Task 2a: (for BLOOM or other multilingual model)__ zero-shot translation. Take the first verse of [Edgar Allan Poe's \"Raven\"](https://www.poetryfoundation.org/poems/48860/the-raven) and __translate it into French.__ (You are free to use any other text of at least the same size)\n",
        "\n",
        "Original text: ```\n",
        "Once upon a midnight dreary, while I pondered, weak and weary,\n",
        "Over many a quaint and curious volume of forgotten lore—\n",
        "    While I nodded, nearly napping, suddenly there came a tapping,\n",
        "As of some one gently rapping, rapping at my chamber door.\n",
        "“’Tis some visitor,” I muttered, “tapping at my chamber door—\n",
        "            Only this and nothing more.” ```\n",
        "\n",
        "Verify your translation by converting french back into english using a public machine translation service.\n",
        "\n",
        "\n",
        "__Task 2b: (non-BLOOM)__ toxicity classification for [SetFit/toxic_conversations](https://huggingface.co/datasets/SetFit/toxic_conversations). Make the model solve binary classification (toxic vs not toxic) in the few shot mode. For few-shot examples, use 2-3 toxic and 2-3 non-toxic non-toxic examples. Measure accuracy on at least 25 samples. You may need to try several different prompts before you find the one that works."
      ],
      "metadata": {
        "id": "Z6Bc13ueKgzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <your code OR writeup OR screenshots>"
      ],
      "metadata": {
        "id": "GygPXoyKKgzo",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "__Task 3 (0.5 pt):__ create a prompt and few-shot examples tha make the model __change the gender pronouns__ of the main actor in a given sentence in any direction of your choice. E.g. the doctor took off _his_ mask <-> the doctor took of _her_ mask.\n"
      ],
      "metadata": {
        "id": "iaYweoPsKgzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <your code OR writeup OR screenshots>"
      ],
      "metadata": {
        "id": "uE-Zv_MoKgzq",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Task 4 (0.5 pt):__ write a prompt and supply examples such that the model would __convert imperial units to metric units__ (miles -> kilometers; mph -> kph). More specifically, the model should rewrite a given sentence and replace all imperial units with their metric equivalents. After it works with basic distances and speed, try to find complicated examples where it does *not* work.\n",
        "\n",
        "Please note that 1 mile is not equal to 1 km :)"
      ],
      "metadata": {
        "id": "bbNrRmgMKgzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <your code OR writeup OR screenshots>"
      ],
      "metadata": {
        "id": "UBxMVGHNKgzr",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: local inference\n",
        "\n",
        "Now, let's try and load the strongest model that can fit a typical Colab GPU (T4 with 16 GB as of spring 2023).\n",
        "\n",
        "Our best candidates are the smaller versions of the best performing open source models:\n",
        "- 8 Bn parameters version of LLaMA\n",
        "- 7 Bn parameters version of [LLaMA](https://arxiv.org/pdf/2302.13971.pdf) - best for spring 2023, released by Facebook\n",
        "- 7 Bn parameters version of [Falcon](https://falconllm.tii.ae) - close competitor to Llama, released in May 2023 by [Technology Innovation Institute of UAE](https://www.tii.ae).\n",
        "- 6.7 Bn parameters version of [OPT](https://arxiv.org/abs/2205.01068) - top choice in this nomination in 2022, released by Facebook.\n",
        "\n",
        "Beware: while these models are smaller than the ones in API, they're still over 60x larger than the BERT we played with last time. The code below will *just barely* fit into memory, so make sure you don't have anything else loaded. Sometimes you may need to restart runtime for the code to work.\n",
        "\n",
        "It's a good time to restart your kernel and switch to GPU! (Runtime -> Change runtime type)\n",
        "<center><img src=\"https://i.imgur.com/OOfDYzJ.png\" width=240px></center>"
      ],
      "metadata": {
        "id": "ZKw-mjuRKgzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers bitsandbytes accelerate sentencepiece optimum auto-gptq\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from tqdm.auto import tqdm, trange\n",
        "assert torch.cuda.is_available(), \"you need cuda for this part\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-03T16:38:07.582929Z",
          "iopub.execute_input": "2025-03-03T16:38:07.583197Z",
          "iopub.status.idle": "2025-03-03T16:38:31.529349Z",
          "shell.execute_reply.started": "2025-03-03T16:38:07.583170Z",
          "shell.execute_reply": "2025-03-03T16:38:31.528607Z"
        },
        "trusted": true,
        "id": "ytALQO97tnTt",
        "outputId": "9ae1b7c8-4116-42b6-e13f-65eb1a49555b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nCollecting transformers\n  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nCollecting accelerate\n  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\nCollecting optimum\n  Downloading optimum-1.24.0-py3-none-any.whl.metadata (21 kB)\nCollecting auto-gptq\n  Downloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (3.3.1)\nCollecting rouge (from auto-gptq)\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nCollecting gekko (from auto-gptq)\n  Downloading gekko-1.2.1-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.14.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.70.16)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.11.12)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto-gptq) (1.17.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.18.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2025.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading optimum-1.24.0-py3-none-any.whl (433 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gekko-1.2.1-py3-none-any.whl (13.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge, transformers, accelerate, gekko, optimum, bitsandbytes, auto-gptq\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.2.1\n    Uninstalling accelerate-1.2.1:\n      Successfully uninstalled accelerate-1.2.1\nSuccessfully installed accelerate-1.4.0 auto-gptq-0.7.1 bitsandbytes-0.45.3 gekko-1.2.1 optimum-1.24.0 rouge-1.0.1 transformers-4.49.0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to use `'meta-llama/Llama-3.2-3B',` you should add the huggungface token. We use a free option"
      ],
      "metadata": {
        "id": "t8VumitqxPg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FOR SECRET ON KAGGLE KERNAL\n",
        "# from kaggle_secrets import UserSecretsClient\n",
        "# user_secrets = UserSecretsClient()\n",
        "# secret_value_0 = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
        "\n",
        "# FOR SECRET ON COLAB\n",
        "# from google.colab import userdata\n",
        "# secret_value_0 = userdata.get('HUGGINGFACE_TOKEN')\n",
        "\n",
        "#Another option:\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-03T16:39:43.631607Z",
          "iopub.execute_input": "2025-03-03T16:39:43.631960Z",
          "iopub.status.idle": "2025-03-03T16:39:44.147510Z",
          "shell.execute_reply.started": "2025-03-03T16:39:43.631935Z",
          "shell.execute_reply": "2025-03-03T16:39:44.146586Z"
        },
        "trusted": true,
        "id": "nN4ULulYtnTu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an example of a simple pipeline, that you don't want in general to use. That's why it's commented"
      ],
      "metadata": {
        "id": "OsKlOczGwrNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline\n",
        "# model_id = 'huggyllama/llama-7b' # \"meta-llama/Llama-3.2-3B\"\n",
        "\n",
        "# pipe = pipeline(\n",
        "#     \"text-generation\",\n",
        "#     model=model_id,\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "#     device_map=\"auto\",\n",
        "#     token=secret_value_0\n",
        "# )\n",
        "\n",
        "# pipe(\"The key to life is\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-03T16:39:54.813014Z",
          "iopub.execute_input": "2025-03-03T16:39:54.813331Z",
          "iopub.status.idle": "2025-03-03T16:39:54.816812Z",
          "shell.execute_reply.started": "2025-03-03T16:39:54.813306Z",
          "shell.execute_reply": "2025-03-03T16:39:54.815932Z"
        },
        "trusted": true,
        "id": "0QrmWnrKtnTu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text generation\n",
        "\n",
        "**Comparison of strategies for language model text generation:**\n",
        "\n",
        "| Strategy | Description | Pros & Cons |\n",
        "| --- | --- | --- |\n",
        "| Greedy Search | Chooses the word with the highest probability as the next word in the sequence. | **Pros:** Simple and fast. <br> **Cons:** Can lead to repetitive and incoherent text. |\n",
        "| Sampling with Temperature | Introduces randomness in the word selection. A higher temperature leads to more randomness. | **Pros:** Allows exploration and diverse output. <br> **Cons:** Higher temperatures can lead to nonsensical outputs. |\n",
        "| Nucleus Sampling (Top-p Sampling) | Selects the next word from a truncated vocabulary, the \"nucleus\" of words that have a cumulative probability exceeding a pre-specified threshold (p). | **Pros:** Balances diversity and quality. <br> **Cons:** Setting an optimal 'p' can be tricky. |\n",
        "| Beam Search | Explores multiple hypotheses (sequences of words) at each step, and keeps the 'k' most likely, where 'k' is the beam width. | **Pros:** Produces more reliable results than greedy search. <br> **Cons:** Can lack diversity and lead to generic responses. |\n",
        "| Top-k Sampling | Randomly selects the next word from the top 'k' words with the highest probabilities. | **Pros:** Introduces randomness, increasing output diversity. <br> **Cons:** Random selection can sometimes lead to less coherent outputs. |\n",
        "| Length Normalization | Prevents the model from favoring shorter sequences by dividing the log probabilities by the sequence length raised to some power. | **Pros:** Makes longer and potentially more informative sequences more likely. <br> **Cons:** Tuning the normalization factor can be difficult. |\n",
        "| Stochastic Beam Search | Introduces randomness into the selection process of the 'k' hypotheses in beam search. | **Pros:** Increases diversity in the generated text. <br> **Cons:** The trade-off between diversity and quality can be tricky to manage. |\n",
        "| Decoding with Minimum Bayes Risk (MBR) | Chooses the hypothesis (out of many) that minimizes expected loss under a loss function. | **Pros:** Optimizes the output according to a specific loss function. <br> **Cons:** Computationally more complex and requires a good loss function. |\n",
        "\n",
        "Documentation references:\n",
        "- [reference for `AutoModelForCausalLM.generate()`](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n",
        "- [reference for `AutoTokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)\n",
        "- Huggingface [docs on generation strategies](https://huggingface.co/docs/transformers/generation_strategies)"
      ],
      "metadata": {
        "id": "5k2zCgAhG7l5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-03T16:39:59.479985Z",
          "iopub.execute_input": "2025-03-03T16:39:59.480388Z",
          "iopub.status.idle": "2025-03-03T16:40:03.291829Z",
          "shell.execute_reply.started": "2025-03-03T16:39:59.480355Z",
          "shell.execute_reply": "2025-03-03T16:40:03.291111Z"
        },
        "trusted": true,
        "id": "IZvFk4uWtnTv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'huggyllama/llama-7b' # 'meta-llama/Llama-3.2-3B'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=device)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=device, token=secret_value_0)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map='auto',\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    offload_state_dict=True,\n",
        "    # token=secret_value_0\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-03T16:40:21.081971Z",
          "iopub.execute_input": "2025-03-03T16:40:21.082418Z",
          "iopub.status.idle": "2025-03-03T16:40:21.086210Z",
          "shell.execute_reply.started": "2025-03-03T16:40:21.082393Z",
          "shell.execute_reply": "2025-03-03T16:40:21.085216Z"
        },
        "trusted": true,
        "id": "kbXnaNvqtnTv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JEr93yGHw7gN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use different generation strategies"
      ],
      "metadata": {
        "id": "AsIcOQqVzJHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GenerationConfig"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-03T16:40:56.091627Z",
          "iopub.execute_input": "2025-03-03T16:40:56.091927Z",
          "iopub.status.idle": "2025-03-03T16:40:56.095460Z",
          "shell.execute_reply.started": "2025-03-03T16:40:56.091906Z",
          "shell.execute_reply": "2025-03-03T16:40:56.094601Z"
        },
        "trusted": true,
        "id": "nTNcniGdtnTv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define generation config\n",
        "generation_config = GenerationConfig(\n",
        "    max_length=200,\n",
        "    temperature=0.8,\n",
        "    top_p=0.9,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "# greedy inference:                                        do_sample=False)\n",
        "# beam search for highest probability:                     num_beams=4)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-03T16:41:01.394195Z",
          "iopub.execute_input": "2025-03-03T16:41:01.394523Z",
          "iopub.status.idle": "2025-03-03T16:41:01.398320Z",
          "shell.execute_reply.started": "2025-03-03T16:41:01.394476Z",
          "shell.execute_reply": "2025-03-03T16:41:01.397397Z"
        },
        "trusted": true,
        "id": "sCojahP4tnTv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'The first discovered martian lifeform looks like' #'The key to life is'\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "print(\"Input batch (encoded):\", batch)\n",
        "\n",
        "output_tokens = model.generate(**batch, generation_config=generation_config)\n",
        "\n",
        "print(\"\\nOutput:\", tokenizer.decode(output_tokens[0].cpu()))"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-27T00:58:29.996175Z",
          "iopub.status.busy": "2025-02-27T00:58:29.995850Z",
          "iopub.status.idle": "2025-02-27T00:58:39.202353Z",
          "shell.execute_reply": "2025-02-27T00:58:39.201632Z",
          "shell.execute_reply.started": "2025-02-27T00:58:29.996153Z"
        },
        "trusted": true,
        "id": "6X47qwK_tnTw",
        "outputId": "f3ee5bcd-b479-48f9-d7b4-ee1fa1503179",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch (encoded): {'input_ids': tensor([[    1,   450,   937, 10943, 14436,   713,  2834,   689,  3430,   763]],\n",
            "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
            "\n",
            "Output: <s> The first discovered martian lifeform looks like a giant, floating jellyfish, and it has tentacles that can be used to transport materials from one location to another.\n",
            "The alien species, dubbed “The Martian Cave Creature,” was first spotted by NASA’s Mars Reconnaissance Orbiter (MRO) spacecraft.\n",
            "According to the researchers, the Martian Cave Creature is a strange-looking jellyfish, and it is the first time that a living organism has been discovered on the Red Planet.\n",
            "The researchers from Arizona State University were the first to spot the Martian Cave Creature and they published their findings in the journal Astrobiology.\n",
            "The researchers suggest that the creature could be a living organism, and it may be using its tentacles to transport materials from one location to another.\n",
            "They believe that the Martian Cave Creature may be the first\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### You can use quntized model's weights (the following code does the quantization on the fly)"
      ],
      "metadata": {
        "id": "Tkbp6YMaz7UF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "h6MYvsy_0JDg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-03T16:41:07.961384Z",
          "iopub.execute_input": "2025-03-03T16:41:07.961716Z",
          "iopub.status.idle": "2025-03-03T16:41:07.972376Z",
          "shell.execute_reply.started": "2025-03-03T16:41:07.961692Z",
          "shell.execute_reply": "2025-03-03T16:41:07.971461Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "# quantization_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True, #or load_in_8bit=True\n",
        "#     bnb_4bit_use_double_quant=True,\n",
        "#     bnb_4bit_quant_type=\"nf4\"\n",
        "#     )"
      ],
      "metadata": {
        "id": "XwkuQWTs0Skn",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-03T16:41:16.247466Z",
          "iopub.execute_input": "2025-03-03T16:41:16.247811Z",
          "iopub.status.idle": "2025-03-03T16:41:16.251366Z",
          "shell.execute_reply.started": "2025-03-03T16:41:16.247786Z",
          "shell.execute_reply": "2025-03-03T16:41:16.250593Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'huggyllama/llama-7b' # 'meta-llama/Llama-3.2-3B'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=device)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=device, token=secret_value_0)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,  #old way: load_in_8bit=True\n",
        "    device_map='auto',\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    offload_state_dict=True,\n",
        "    # token=secret_value_0\n",
        "    )"
      ],
      "metadata": {
        "id": "OjdBzI1R0OkP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-03T17:15:33.601179Z",
          "iopub.execute_input": "2025-03-03T17:15:33.601532Z",
          "iopub.status.idle": "2025-03-03T17:15:33.605430Z",
          "shell.execute_reply.started": "2025-03-03T17:15:33.601475Z",
          "shell.execute_reply": "2025-03-03T17:15:33.604604Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Or you can upload a quantized model"
      ],
      "metadata": {
        "id": "EwFQ1PtE6efs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"TheBloke/Llama-2-13B-GPTQ\" #meta-llama/Llama-3-8B-GPTQ\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True)\n",
        "#model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=False, revision=\"main\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-03T17:10:06.039565Z",
          "iopub.execute_input": "2025-03-03T17:10:06.040000Z",
          "iopub.status.idle": "2025-03-03T17:10:06.043376Z",
          "shell.execute_reply.started": "2025-03-03T17:10:06.039951Z",
          "shell.execute_reply": "2025-03-03T17:10:06.042601Z"
        },
        "id": "cUvi-bsP6efs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'The first discovered martian lifeform looks like' #'The key to life is'\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "print(\"Input batch (encoded):\", batch)\n",
        "\n",
        "output_tokens = model.generate(**batch, generation_config=generation_config)\n",
        "\n",
        "print(\"\\nOutput:\", tokenizer.decode(output_tokens[0].cpu()))\n",
        "#print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-03T16:59:12.015251Z",
          "iopub.execute_input": "2025-03-03T16:59:12.015972Z",
          "iopub.status.idle": "2025-03-03T17:04:21.834033Z",
          "shell.execute_reply.started": "2025-03-03T16:59:12.015938Z",
          "shell.execute_reply": "2025-03-03T17:04:21.833240Z"
        },
        "trusted": true,
        "id": "Mws9fsY4tnTw",
        "outputId": "c9327536-dc06-4b2b-e501-b862fede46ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Input batch (encoded): {'input_ids': tensor([[    1,   450,   937, 10943, 14436,   713,  2834,   689,  3430,   763]],\n       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n\nOutput: <s> The first discovered martian lifeform looks like a giant mutant spider\nA new study suggests that the first life to colonize Mars was a spider-like creature.\nA spider-like creature? Really?\nOkay, it’s not really a spider. It’s actually a creature with spider-like features.\nThe first life to colonize Mars may have been a spider-like creature, according to a new study by researchers at the University of Oxford.\nThis creature is called the “spider-like creature” because it has eight legs, two eyes, and a body that resembles a spider.\nThe creature is estimated to be around 100 million years old, making it one of the oldest known lifeforms on Mars.\nThe study also suggests that the spider-like creature may have been able to reproduce on Mars, which would make it the first known example of Martian life\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Low-level code for text generation"
      ],
      "metadata": {
        "id": "P17ehC1sKgzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You may need to clear GPU memory\n",
        "# import gc\n",
        "# del output_tokens\n",
        "# del model\n",
        "# del tokenizer\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-03T17:12:20.101763Z",
          "iopub.execute_input": "2025-03-03T17:12:20.102058Z",
          "iopub.status.idle": "2025-03-03T17:12:21.287797Z",
          "shell.execute_reply.started": "2025-03-03T17:12:20.102037Z",
          "shell.execute_reply": "2025-03-03T17:12:21.286913Z"
        },
        "id": "fOGigkRb6eft"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'huggyllama/llama-7b' # 'meta-llama/Llama-3.2-3B'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map='auto',\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    offload_state_dict=True\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-03T17:12:38.008944Z",
          "iopub.execute_input": "2025-03-03T17:12:38.009234Z",
          "iopub.status.idle": "2025-03-03T17:13:25.786351Z",
          "shell.execute_reply.started": "2025-03-03T17:12:38.009213Z",
          "shell.execute_reply": "2025-03-03T17:13:25.785500Z"
        },
        "trusted": true,
        "id": "AmnmqNXStnTw",
        "outputId": "94bedd9d-e327-4709-9280-14c5d6e696be",
        "colab": {
          "referenced_widgets": [
            "6ffdc8c7f0a54c288d5dd2a13a445fd5"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ffdc8c7f0a54c288d5dd2a13a445fd5"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"New York is the capital of\"\n",
        "# prompt = \"Skippy, a young android, likes to dream about electric\"\n",
        "\n",
        "print(prompt, '\\n')\n",
        "\n",
        "voc = tokenizer.get_vocab()\n",
        "voc_rev = {v:k for k, v in voc.items()}  # reverse vocab for decode\n",
        "\n",
        "for i in range(10):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "    logits = model.forward(**inputs).logits[0, -1, :]\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    next_token_id = torch.multinomial(probs.flatten(), num_samples=1)\n",
        "\n",
        "    next_token = tokenizer.decode(next_token_id)\n",
        "    prompt += next_token\n",
        "\n",
        "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "    top_tokens = sorted_indices[:5]\n",
        "    print(f\"Step #{i} candidates:\")\n",
        "    for t, p in zip (top_tokens, sorted_probs):\n",
        "        t = voc_rev[t.item()]\n",
        "        print(f\"{t:<10}: {p:.4f} \")\n",
        "\n",
        "    print(f'\\nChosen token: {next_token}', end='\\n\\n', flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2025-03-03T17:13:31.214713Z",
          "iopub.execute_input": "2025-03-03T17:13:31.214995Z",
          "iopub.status.idle": "2025-03-03T17:13:32.841448Z",
          "shell.execute_reply.started": "2025-03-03T17:13:31.214975Z",
          "shell.execute_reply": "2025-03-03T17:13:32.840768Z"
        },
        "id": "LZJvOMbmG7l8",
        "outputId": "1fe924b7-3b66-41b9-99c0-16619b064035",
        "tags": [],
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "New York is the capital of \n\nStep #0 candidates:\n▁the      : 0.6235 \n▁fashion  : 0.0363 \n▁America  : 0.0318 \n▁New      : 0.0196 \n▁American : 0.0114 \n\nChosen token: the\n\nStep #1 candidates:\n▁United   : 0.3359 \n▁world    : 0.1368 \n▁USA      : 0.0919 \n▁state    : 0.0699 \n▁fashion  : 0.0448 \n\nChosen token: United\n\nStep #2 candidates:\n▁States   : 0.9302 \nSt        : 0.0390 \n▁states   : 0.0064 \n▁Stat     : 0.0041 \n▁State    : 0.0034 \n\nChosen token: St\n\nStep #3 candidates:\nates      : 0.9976 \nated      : 0.0010 \nat        : 0.0002 \na         : 0.0002 \naten      : 0.0002 \n\nChosen token: ates\n\nStep #4 candidates:\n.         : 0.2803 \nof        : 0.2454 \n,         : 0.1387 \nand       : 0.1072 \n▁of       : 0.0650 \n\nChosen token: of\n\nStep #5 candidates:\nAmerica   : 0.8906 \n▁America  : 0.0854 \nAmerican  : 0.0062 \nAmer      : 0.0048 \nA         : 0.0020 \n\nChosen token: America\n\nStep #6 candidates:\n.         : 0.5327 \n,         : 0.1730 \nand       : 0.1227 \n▁and      : 0.0458 \n(         : 0.0169 \n\nChosen token: ,\n\nStep #7 candidates:\n▁and      : 0.1500 \n▁the      : 0.1488 \nthe       : 0.0648 \nand       : 0.0583 \n▁a        : 0.0409 \n\nChosen token: a\n\nStep #8 candidates:\n▁country  : 0.1595 \n▁federal  : 0.0854 \n▁large    : 0.0376 \n▁state    : 0.0337 \n▁very     : 0.0237 \n\nChosen token: country\n\nStep #9 candidates:\n▁in       : 0.0952 \nin        : 0.0702 \nlocated   : 0.0564 \n▁that     : 0.0549 \n▁located  : 0.0538 \n\nChosen token: located\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Chain-of-thought prompting (4 points total)\n",
        "\n",
        "![img](https://github.com/kojima-takeshi188/zero_shot_cot/raw/main/img/image_stepbystep.png)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5ZaQZhPXOPSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import locale; locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!wget https://raw.githubusercontent.com/kojima-takeshi188/zero_shot_cot/2824685e25809779dbd36900a69825068e9f51ef/dataset/AQuA/test.json -O aqua.json\n",
        "data = list(map(json.loads, open(\"aqua.json\")))"
      ],
      "metadata": {
        "id": "N2AmfelTn5en",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Example:\")\n",
        "data[150]"
      ],
      "metadata": {
        "id": "IATXmPfYw8s6",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive solution\n",
        "\n",
        "Here, we prompt the model to choose an answer to the example above (`data[150]`) out of the options given above. We're using a format that mimics grade school solution textbook.\n",
        "\n",
        "Please note that there are minor formatting changes in options: an extra space and an opening bracket. Those may or may not be important :)"
      ],
      "metadata": {
        "id": "6UcOYQPW8sVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXAMPLE_0SHOT = \"\"\"\n",
        "Question: Janice bikes at 10 miles per hour, while Jennie bikes at 20. How long until they have collectively biked 1 mile?\n",
        "Answer Choices: (A) 1 minute (B) 2 minutes (C) 3 minutes (D) 4 minutes (E) 5 minutes\n",
        "Correct Answer:\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "KtkkdiJl3-UI",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# solving an equation directly\n",
        "batch = tokenizer(EXAMPLE_0SHOT, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "torch.manual_seed(1337)\n",
        "output_tokens = model.generate(**batch, max_new_tokens=100, do_sample=True, top_p=0.9)\n",
        "print(\"[Prompt:]\\n\" + EXAMPLE_0SHOT)\n",
        "print(\"=\" * 80)\n",
        "print(\"[Generated:]\", tokenizer.decode(output_tokens[0][batch['input_ids'].shape[1]:].cpu()))"
      ],
      "metadata": {
        "id": "hyQ_8tJc6nyv",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here's how you can solve this with few-shot chain-of-thought prompting.\n",
        "\n",
        "You need to chang 3 things\n",
        "- use a new field called **Rationale**, that contains a step-by-step solution to the problem\n",
        "- add several few-shot examples of previously solved problems **with rationales**\n",
        "- change the final prompt so that the model has to generate rationale before answering"
      ],
      "metadata": {
        "id": "suSkiDk28I6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXAMPLE_3SHOT_CHAIN_OF_THOUGHT = \"\"\"\n",
        "Question: The original retail price of an appliance was 60 percent more than its wholesale cost. If the appliance was actually sold for 20 percent less than the original retail price, then it was sold for what percent more than its wholesale cost?\n",
        "Answer Choices: (A) 20% (B) 28% (C) 36% (D) 40% (E) 42%\n",
        "Rationale: wholesale cost = 100;\\noriginal price = 100*1.6 = 160;\\nactual price = 160*0.8 = 128.\\nAnswer: B.\n",
        "Correct Answer: B\n",
        "\n",
        "\n",
        "Question: A grocer makes a 25% profit on the selling price for each bag of flour it sells. If he sells each bag for $100 and makes $3,000 in profit, how many bags did he sell?\n",
        "Answer Choices: (A) 12 (B) 16 (C) 24 (D) 30 (E) 40\n",
        "Rationale: Profit on one bag: 100*1.25= 125\\nNumber of bags sold = 3000/125 = 24\\nAnswer is C.\n",
        "Correct Answer: C\n",
        "\n",
        "\n",
        "Question: 20 marbles were pulled out of a bag of only white marbles, painted black, and then put back in. Then, another 20 marbles were pulled out, of which 1 was black, after which they were all returned to the bag. If the percentage of black marbles pulled out the second time represents their percentage in the bag, how many marbles in total Q does the bag currently hold?\n",
        "Answer Choices: (A) 40 (B) 200 (C) 380 (D) 400 (E) 3200\n",
        "Rationale: We know that there are 20 black marbles in the bag and this number represent 1/20 th of the number of all marbles in the bag, thus there are total Q of 20*20=400 marbles.\\nAnswer: D.\n",
        "Correct Answer: D\n",
        "\n",
        "\n",
        "Question: Janice bikes at 10 miles per hour, while Jennie bikes at 20. How long until they have collectively biked 1 mile?\n",
        "Answer Choices: (A) 1 minute (B) 2 minutes (C) 3 minutes (D) 4 minutes (E) 5 minutes\n",
        "Rationale:\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "K0F1jYdRvoJW",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "batch = tokenizer(EXAMPLE_3SHOT_CHAIN_OF_THOUGHT, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "torch.manual_seed(1337)\n",
        "output_tokens = model.generate(**batch, max_new_tokens=100, do_sample=True, top_p=0.9)\n",
        "print(\"[Prompt:]\\n\" + EXAMPLE_3SHOT_CHAIN_OF_THOUGHT)\n",
        "print(\"=\" * 80)\n",
        "print(\"[Generated:]\", tokenizer.decode(output_tokens[0][batch['input_ids'].shape[1]:].cpu()))\n",
        "#### NOTE: scroll down for the final answer (below the ======= line)"
      ],
      "metadata": {
        "id": "Tn8QoAYcRkHC",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Task 5 (3 pt)__ write a function that automatically creates chain-of-thought prompts. Follow the instructions from the function docstring."
      ],
      "metadata": {
        "id": "s4px3jv-99-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION_PREFIX = \"Question: \"\n",
        "OPTIONS_PREFIX = \"Answer Choices: \"\n",
        "CHAIN_OF_THOUGHT_PREFIX = \"Rationale: \"\n",
        "ANSWER_PREFIX = \"Correct Answer: \"\n",
        "FEWSHOT_SEPARATOR = \"\\n\\n\\n\"\n",
        "\n",
        "def make_prompt(*, main_question, fewshot_examples):\n",
        "  \"\"\"\n",
        "  Your goal is to produce the same prompt as the EXAMPLE_3SHOT_CHAIN_OF_THOUGHT automatically\n",
        "\n",
        "  For each few-shot question, make sure to follow the following rules:\n",
        "  1. Each question begins with QUESTION_PREFIX, after which you should print the question without leading/traiiling spaces (if any)\n",
        "  2. After the question, provide space-separated options. Each option should be put in double brackets, followed by option text, e.g. \"(A) 146%\"\n",
        "  3. Then, provide the answer as a single letter (A-E)\n",
        "  4. Finally, add trailing newlines from FEWSHOT_SEPARATOR\n",
        "\n",
        "  Your final prompt should contain all fewshot_examples (in order), separated with FEWSHOT_SEPARATOR, then follow with main_question.\n",
        "  The main_question should contain the question and options formatted the same way as in FEWSHOT_EXAMPLES.\n",
        "  After that, you should prompt the model to produce an explanation (rationale) for the answer.\n",
        "\n",
        "  Please make sure your prompt contains no leading/trailing newlines or spaces, same as in EXAMPLE_3SHOT_CHAIN_OF_THOUGHT\n",
        "  \"\"\"\n",
        "\n",
        "  <YOUR CODE HERE>\n",
        "\n",
        "  return <a string that contains the prompt formatted as per instructions above>\n",
        "\n",
        "\n",
        "\n",
        "generated_fewshot_prompt = make_prompt(main_question=data[150], fewshot_examples=(data[30], data[20], data[5]))\n",
        "assert generated_fewshot_prompt == EXAMPLE_3SHOT_CHAIN_OF_THOUGHT, \"prompts don't match\"\n",
        "assert generated_fewshot_prompt != make_prompt(main_question=data[150], fewshot_examples=())\n",
        "assert generated_fewshot_prompt.endswith(make_prompt(main_question=data[150], fewshot_examples=()))\n",
        "\n",
        "print(\"Well done!\")\n",
        "\n",
        "# Hint: if two prompts do not match, you may find it usefull to use https://www.diffchecker.com or similar to find the difference"
      ],
      "metadata": {
        "id": "_ntyFPMt9fyt",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Task 6 (3 points):__ Evaluate your prompt.\n",
        "\n",
        "Please run the model on the entire dataset and measure it's accuracy.\n",
        "For each question, peak $n=5$ other questions at random to serve as few-shot examples. Make sure not to accidentally sample the main_question among few-shot examples. For scientific evaluation, it is also a good practice to split the data into two parts: one for eval, and another for few-shot examples. However, doing so is optional in this homework.\n",
        "\n",
        "The tricky part is when to stop generating: if you don't control for this, your model can accidentally generate a whole new question - and promptyly answer it :) To make sure you get the correct answer, stop generating tokens when the model is done explaining it's solution. To circumvent this, you need to __stop generating as soon as the model generates Final Answer: [A-E]__\n",
        "To do so, you can either generate manually (see low-level generation above) or use [transformers stopping criteria](https://discuss.huggingface.co/t/implimentation-of-stopping-criteria-list/20040/2), whichever you prefer.\n",
        "\n",
        "If you do everything right, the model should be much better than random. However, please __do not expect miracles__: this is far from the best models, and it will perform much worse than an average human."
      ],
      "metadata": {
        "id": "P7DzQ8hfOcFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SAMPLES = 0    # use this to count how many samples you evaluated\n",
        "NUM_RESPONDED = 0  # how many times did the model produce Correct Answer: (letter) in it's response. use as a sanity check.\n",
        "NUM_CORRECT = 0    # how many times did the model's chosen answer (letter) match the correct answer"
      ],
      "metadata": {
        "id": "oMvj9eCtQwnz",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "< A whole lot of your code here >\n",
        "\n",
        "# Optionally, consider inferencing multiple sentences in a batch for faster inference;\n",
        "# If you choose to batch outputs, make sure the results are the same as with batch=1 (using greedy inference)"
      ],
      "metadata": {
        "id": "Zh6gejr0JNuh",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Responded %%:\", NUM_RESPONDED / NUM_SAMPLES)\n",
        "print(\"Accuracy (when responded):\", NUM_CORRECT / NUM_RESPONDED)\n",
        "print(\"Accuracy (overall):\", NUM_CORRECT / NUM_SAMPLES)\n",
        "\n",
        "if NUM_RESPONDED / NUM_SAMPLES < 0.9:\n",
        "  print(\"Something is wrong with the evaluation technique (for 5-shot CoT): the model refuses to answer too many questions.\")\n",
        "  print(\"Make sure you generate enough tokens that the model can produce a correct answer.\")\n",
        "  print(\"When in doubt, take a look at the full model output. You can often spot errors there.\")"
      ],
      "metadata": {
        "id": "gxrCJvxIJSjA",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Task 7 (2 points)__ Experiment time!\n",
        "<img width=200px src=https://www.evolvefish.com/cdn-cgi/image/quality%3D85/assets/images/Apparel/TShirtsWomenCont/Main/EF-APP-CWT-00068(Main).jpg>\n",
        "\n",
        "Your final quest is to use the testbench you've just written to answer one of the following questions:\n",
        "\n",
        "### Option 1: How many shots do you need?\n",
        "\n",
        "How does model accuracy change with the number of fewshot examples?\n",
        "\n",
        "a. check if the model accuracy changes as you increase/decrease the number of \"shots\"\n",
        "\n",
        "b. try to prompt-engineer a model into giving the best rationale __without__ any few-shot examples, i.e. zero-shot\n",
        "\n",
        "For zero-shot mode, feel free to use wild prompt-engineering or modify the inference procedure.\n",
        "\n",
        "### Option 2: Is this prompting tecnique reliable?\n",
        "\n",
        "_Inspired by ongoing research by Anton Voronov, Lena Volf and Max Ryabinin._\n",
        "\n",
        "For this option, you need to check if the model behavior (and hence, accuracy) is robust to perturbations in the input prompt.\n",
        "\n",
        "a. Does the accuracy degrade if you provide wrong answers to few-shot examples? (make sure to modify rationale if it contains answer in the end)\n",
        "\n",
        "b. Does it degrade if you replace question/answer prompts with \"Q\" and \"A\"? What if you write both on the same line? Change few-shot separators?\n",
        "\n",
        "\n",
        "\n",
        "### Option 3: Inference Matters\n",
        "\n",
        "There are many ways to inference the model, not all of them equal.\n",
        "\n",
        "a. check whether greedy inference or beam search affects model generation quality\n",
        "\n",
        "b. implement and evaluate sampling with voting (see explanation below).\n",
        "\n",
        "\n",
        "The voting technique(b) should work as follows: first, you generate k (e.g. 50) \"attempts\" at an answer using nucleus sampling (or a similar technique).\n",
        "Then, you count how many of those attempts chose a particular option (A, B, etc) as the final answer. The option that was chosen most frequently has the most \"votes\", and therefore \"wins\".\n",
        "\n",
        "To speed up voting, you may want to generate these attempts in parallel as a batch. That should be very easy to implement: just run `model.generate` on a list with multiple copies of the same prompt.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "================================================\n",
        "\n",
        "__Common rules:__ You will need to test both hypothes (A and B) in the chosen option. You may choose to replace one of them with your own idea - but please ask course staff in advance (via telegram) if you want full points.\n",
        "\n",
        "Feel free to organize your code and report as you see fit - but please make sure it's readable and the code runs top-to-bottom :)\n",
        "Write a short informal report about what you tried and, in doing so, what did you found. Minimum of 2 paragraphs; more is ok; creative visualizations are welcome.\n",
        "\n",
        "You are allowed (but not required) to prompt the model into generating a report for you --- or helping you write one. However, if you do so, make sure that it is still human-readable :)\n",
        "\n"
      ],
      "metadata": {
        "id": "UZLK2rLiKxbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feel free to organize your solution as you see it"
      ],
      "metadata": {
        "id": "_r6UVDl4NEua",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}