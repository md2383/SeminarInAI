{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/md2383/SeminarInAI/blob/main/Week01_embeddings/Practice1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbCB4Dck226w"
      },
      "source": [
        "## Practice 1: Fun with Word Embeddings (2 points)\n",
        "\n",
        "Today we gonna play with word embeddings: train our own little embeddings, load one from gensim model zoo and use it to visualize text corpora.\n",
        "\n",
        "This whole thing is gonna happen on top of embedding dataset.\n",
        "\n",
        "__Requirements:__  `pip install --upgrade nltk gensim bokeh` , but only if you're running locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xz-wBGtI226x",
        "outputId": "93c4eb26-89c3-4e3c-df60-769be78d49d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-15 02:50:25--  https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/p0t2dw6oqs6oxpd6zz534/quora.txt?rlkey=bjupppwua4zmd4elz8octecy9&dl=1 [following]\n",
            "--2025-01-15 02:50:25--  https://www.dropbox.com/scl/fi/p0t2dw6oqs6oxpd6zz534/quora.txt?rlkey=bjupppwua4zmd4elz8octecy9&dl=1\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc6656d35a16296393c8075b3ddc.dl.dropboxusercontent.com/cd/0/inline/CiMYsyKUkPisSnbAsbVSUWFBeFLwZPNKMO7eR9M11Y7tBR0O-tpj2vyJbMFnu5TqwQkFzBJv0ez-4kn8akynzoeU2t_09Dys7D11JgWazQYsEEvLF80krHt68D97NM_3k5s/file?dl=1# [following]\n",
            "--2025-01-15 02:50:26--  https://uc6656d35a16296393c8075b3ddc.dl.dropboxusercontent.com/cd/0/inline/CiMYsyKUkPisSnbAsbVSUWFBeFLwZPNKMO7eR9M11Y7tBR0O-tpj2vyJbMFnu5TqwQkFzBJv0ez-4kn8akynzoeU2t_09Dys7D11JgWazQYsEEvLF80krHt68D97NM_3k5s/file?dl=1\n",
            "Resolving uc6656d35a16296393c8075b3ddc.dl.dropboxusercontent.com (uc6656d35a16296393c8075b3ddc.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc6656d35a16296393c8075b3ddc.dl.dropboxusercontent.com (uc6656d35a16296393c8075b3ddc.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33813903 (32M) [application/binary]\n",
            "Saving to: ‘./quora.txt’\n",
            "\n",
            "./quora.txt         100%[===================>]  32.25M  52.0MB/s    in 0.6s    \n",
            "\n",
            "2025-01-15 02:50:27 (52.0 MB/s) - ‘./quora.txt’ saved [33813903/33813903]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download the data:\n",
        "!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt\n",
        "# alternative download link: https://yadi.sk/i/BPQrUu1NaTduEw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "_cSxVOdg226y",
        "outputId": "a0487b3b-f14d-46df-bb38-8518edf100df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"What TV shows or books help you read people's body language?\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "with open(\"./quora.txt\", encoding=\"utf-8\") as file:\n",
        "    data = list(file)\n",
        "\n",
        "data[50]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_6I_Krl226y"
      },
      "source": [
        "__Tokenization:__ a typical first step for an NLP task is to split raw data into words.\n",
        "The text we're working with has punctuation and emoticons attached to some words, so a simple str.split won't work.\n",
        "\n",
        "Let's use __`nltk`__ - a library that handles many NLP tasks like tokenization, stemming or part-of-speech tagging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyNJ2gZs226y",
        "outputId": "e8625df2-2f63-46ac-ba1f-ff6ba30017f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What', 'TV', 'shows', 'or', 'books', 'help', 'you', 'read', 'people', \"'\", 's', 'body', 'language', '?']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokenizer = WordPunctTokenizer()\n",
        "\n",
        "print(tokenizer.tokenize(data[50]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YzTsS2Yy226z"
      },
      "outputs": [],
      "source": [
        "# TASK: lowercase everything and extract tokens with tokenizer.\n",
        "# data_tok should be a list of lists of tokens for each line in data.\n",
        "\n",
        "data_tok = (tokenizer.tokenize(x.lower()) for x in data)# YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iveYXKIW226z"
      },
      "outputs": [],
      "source": [
        "assert all(isinstance(row, (list, tuple)) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
        "assert all(all(isinstance(tok, str) for tok in row) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
        "is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n",
        "assert all(map(lambda l: not is_latin(l) or l.islower(), map(' '.join, data_tok))), \"please make sure to lowercase the data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SewqnoBd226z",
        "outputId": "15daca56-bf74-4d61-edd5-d6e16fbeabb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"can i get back with my ex even though she is pregnant with another guy ' s baby ?\", 'what are some ways to overcome a fast food addiction ?']\n"
          ]
        }
      ],
      "source": [
        "print([' '.join(row) for row in data_tok[:2]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueu43HXO226z"
      },
      "source": [
        "__Word vectors:__ as the saying goes, there's more than one way to train word embeddings. There's Word2Vec and GloVe with different objective functions. Then there's fasttext that uses character-level models to train word embeddings.\n",
        "\n",
        "The choice is huge, so let's start with something small: __gensim__ is another nlp library that features many vector-based models incuding word2vec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uONU58ZH226z"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(data_tok,\n",
        "                 vector_size=32,      # embedding vector size\n",
        "                 min_count=5,  # consider words that occured at least 5 times\n",
        "                 window=5).wv  # define context as a 5-word window around the target word\n",
        "\n",
        "# From gensim docs\n",
        "# vw: This object essentially contains the mapping between words and embeddings.\n",
        "# After training, it can be used directly to query those embeddings in various ways."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zd274e17226z"
      },
      "outputs": [],
      "source": [
        "# now you can get word vectors !\n",
        "model.get_vector('anything')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bcdi_vUw2260"
      },
      "outputs": [],
      "source": [
        "# or query similar words directly. Go play with it!\n",
        "model.most_similar('bread')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zgwk4rUb2260"
      },
      "source": [
        "### Using pre-trained model\n",
        "\n",
        "Now, imagine training real life-sized (100~300D) word embeddings on gigabytes of text: wikipedia articles or twitter posts.\n",
        "\n",
        "Thankfully, nowadays you can get a pre-trained word embedding model in 2 lines of code (no sms required, promise).\n",
        "\n",
        "After being downloaded for the first time (or if you manually delete it), the model is saved in the `~/gensim_data` or `%USER_PATH%/gensim_data` directory. This can be checked seting the return_path parameter to True."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MePXiL-o2260"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api.info()"
      ],
      "metadata": {
        "id": "C7U5bQMS7Pj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = api.load('glove-twitter-100')"
      ],
      "metadata": {
        "id": "NQV4HZuR7VXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1A5jfo92260"
      },
      "outputs": [],
      "source": [
        "model.most_similar(positive=[\"coder\", \"money\"], negative=[\"brain\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIquSyBQ2260"
      },
      "source": [
        "### Visualizing word vectors\n",
        "\n",
        "One way to see if our vectors are good is to plot them. Those vectors are in 30D+ space, but we humans are used to 2D or 3D.\n",
        "\n",
        "Luckily, we machine learners know about __dimensionality reduction__ methods.\n",
        "\n",
        "Let's use that knowledge to plot 1000 most frequent words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_iQE0JA2260"
      },
      "outputs": [],
      "source": [
        "words = model.index_to_key[:1000]\n",
        "\n",
        "print(words[::100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfJeb3rv2260"
      },
      "outputs": [],
      "source": [
        "# for each word, compute it's vector with model\n",
        "word_vectors = # YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wNzioXxU2261"
      },
      "outputs": [],
      "source": [
        "assert isinstance(word_vectors, np.ndarray)\n",
        "assert word_vectors.shape == (len(words), 100)\n",
        "assert np.isfinite(word_vectors).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwMmhm_m2261"
      },
      "source": [
        "#### Linear projection: PCA\n",
        "\n",
        "The simplest linear dimensionality reduction method is **P**rincipial **C**omponent **A**nalysis.\n",
        "\n",
        "In geometric terms, PCA tries to find axes along which most of the variance occurs. The \"natural\" axes, if you wish.\n",
        "\n",
        "<img src=\"https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/pca_fish.png\" style=\"width:30%\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "O6nt6XSG2261"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# map word vectors onto 2d plane with PCA. Use good old sklearn api (fit, transform)\n",
        "# after that, normalize vectors to make sure they have zero mean and unit variance\n",
        "word_vectors_pca =  # YOUR CODE\n",
        "\n",
        "# and maybe MORE OF YOUR CODE here :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Dpoh2J382261"
      },
      "outputs": [],
      "source": [
        "assert word_vectors_pca.shape == (len(word_vectors), 2), \"there must be a 2d vector for each word\"\n",
        "assert max(abs(word_vectors_pca.mean(0))) < 1e-5, \"points must be zero-centered\"\n",
        "assert max(abs(1.0 - word_vectors_pca.std(0))) < 1e-2, \"points must have unit variance\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v35sQYLP2261"
      },
      "source": [
        "#### Let's draw it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dy1BmXti2261"
      },
      "outputs": [],
      "source": [
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()\n",
        "\n",
        "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
        "                 width=600, height=400, show=True, **kwargs):\n",
        "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
        "    if isinstance(color, str): color = [color] * len(x)\n",
        "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
        "\n",
        "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
        "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
        "\n",
        "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
        "    if show: pl.show(fig)\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6I2Bnivm2261"
      },
      "outputs": [],
      "source": [
        "draw_vectors(word_vectors_pca[:, 0], word_vectors_pca[:, 1], token=words)\n",
        "\n",
        "# hover a mouse over there and see if you can identify the clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8eD22FV2261"
      },
      "source": [
        "### Visualizing neighbors with t-SNE\n",
        "PCA is nice but it's strictly linear and thus only able to capture coarse high-level structure of the data.\n",
        "\n",
        "If we instead want to focus on keeping neighboring points near, we could use TSNE, which is itself an embedding method. Here you can play __[with TSNE](https://projector.tensorflow.org/)__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijtSeFIV2261"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# map word vectors onto 2d plane with TSNE. hint: don't panic it may take a minute or two to fit.\n",
        "# normalize them as just lke with pca\n",
        "\n",
        "\n",
        "word_tsne = #YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": false,
        "id": "HGpzfK5U2262"
      },
      "outputs": [],
      "source": [
        "draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78zv78i92262"
      },
      "source": [
        "### Visualizing phrases\n",
        "\n",
        "Word embeddings can also be used to represent short phrases. The simplest way is to take __an average__ of vectors for all tokens in the phrase with some weights.\n",
        "\n",
        "This trick is useful to identify what data are you working with: find if there are any outliers, clusters or other artefacts.\n",
        "\n",
        "Let's try this new hammer on our data!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eJ6uyhwP2262"
      },
      "outputs": [],
      "source": [
        "def get_phrase_embedding(phrase):\n",
        "    \"\"\"\n",
        "    Convert phrase to a vector by aggregating it's word embeddings. See description above.\n",
        "    \"\"\"\n",
        "    # 1. lowercase phrase\n",
        "    # 2. tokenize phrase\n",
        "    # 3. average word vectors for all words in tokenized phrase\n",
        "    # skip words that are not in model's vocabulary\n",
        "    # if all words are missing from vocabulary, return zeros\n",
        "\n",
        "    vector = np.zeros([model.vector_size], dtype='float32')\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    return vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "59WUh_Cw2262"
      },
      "outputs": [],
      "source": [
        "vector = get_phrase_embedding(\"I'm very sure. This never happened to me before...\")\n",
        "\n",
        "assert np.allclose(vector[::10],\n",
        "                   np.array([ 0.31807372, -0.02558171,  0.0933293 , -0.1002182 , -1.0278689 ,\n",
        "                             -0.16621883,  0.05083408,  0.17989802,  1.3701859 ,  0.08655966],\n",
        "                              dtype=np.float32))\n",
        "assert np.array_equal(get_phrase_embedding(\"thisisgibberish\"), np.zeros([model.vector_size], dtype='float32')), \"corner case for all missing words should be handled as described in the function comments\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RMr-1ta92262"
      },
      "outputs": [],
      "source": [
        "# let's only consider ~5k phrases for a first run.\n",
        "chosen_phrases = data[::len(data) // 1000]\n",
        "\n",
        "# compute vectors for chosen phrases\n",
        "phrase_vectors = # YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FVkWcNDgGa2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UNHaP-v52262"
      },
      "outputs": [],
      "source": [
        "assert isinstance(phrase_vectors, np.ndarray) and np.isfinite(phrase_vectors).all()\n",
        "assert phrase_vectors.shape == (len(chosen_phrases), model.vector_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ItSVb-5K2262"
      },
      "outputs": [],
      "source": [
        "# map vectors into 2d space with pca, tsne or your other method of choice\n",
        "# don't forget to normalize\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkyLe0dM2263"
      },
      "source": [
        "Finally, let's build a simple \"similar question\" engine with phrase embeddings we've built."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hMKXlIO82263"
      },
      "outputs": [],
      "source": [
        "# compute vector embedding for all lines in data\n",
        "data_vectors ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CwND5OfL2263"
      },
      "outputs": [],
      "source": [
        "def find_nearest(query, k=10):\n",
        "    \"\"\"\n",
        "    given text line (query), return k most similar lines from data, sorted from most to least similar\n",
        "    similarity should be measured as cosine between query and line embedding vectors\n",
        "    hint: it's okay to use global variables: data and data_vectors. see also: np.argpartition, np.argsort\n",
        "    \"\"\"\n",
        "    # YOUR CODE\n",
        "    return  # <YOUR CODE: top-k lines starting from most similar>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VyFO2CY62263"
      },
      "outputs": [],
      "source": [
        "results = find_nearest(query=\"How do i enter the matrix?\", k=10)\n",
        "\n",
        "print(''.join(results))\n",
        "\n",
        "assert len(results) == 10 and isinstance(results[0], str)\n",
        "assert results[0] == 'How do I get to the dark web?\\n'\n",
        "assert results[3] == 'What can I do to save the world?\\n'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gOsDY-ue2267"
      },
      "outputs": [],
      "source": [
        "find_nearest(query=\"How does Trump?\", k=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "S2WarUzw2268"
      },
      "outputs": [],
      "source": [
        "find_nearest(query=\"Why don't i ask a question myself?\", k=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "OvIr6AOd2268"
      },
      "source": [
        "__Now what?__\n",
        "* Try running TSNE on all data, not just 1000 phrases\n",
        "* See what other embeddings are there in the model zoo: `gensim.downloader.info()`\n",
        "* Take a look at [FastText](https://github.com/facebookresearch/fastText) embeddings\n",
        "* Optimize `find_nearest` with locality-sensitive hashing: use [nearpy](https://github.com/pixelogik/NearPy) or `sklearn.neighbors`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}