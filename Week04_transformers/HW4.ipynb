{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmsFABwClrsS"
      },
      "source": [
        "## Attention Model  (10 pt)\n",
        "\n",
        "Credit to https://github.com/yandexdataschool/nlp_course/blob/2023/week04_seq2seq/practice_and_homework_pytorch.ipynb\n",
        "\n",
        "In previous notebook we composed encoder-decoder recurrent neural networks and applied it to the task of machine translation.\n",
        "\n",
        "![img](https://esciencegroup.files.wordpress.com/2016/03/seq2seq.jpg)\n",
        "_(img: esciencegroup.files.wordpress.com)_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4N9AD2dlrsU"
      },
      "source": [
        "## Our task today to add additive attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKSszqZLsHCe"
      },
      "outputs": [],
      "source": [
        "#We'll use data from https://www.kaggle.com/datasets/devicharith/language-translation-englishfrench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klOUlqvHWM3D"
      },
      "outputs": [],
      "source": [
        "#!kaggle datasets download -d devicharith/language-translation-englishfrench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgNi2BF5GVeN",
        "outputId": "7ec56f03-d9a0-40c4-ef57-de3fc7ffe7a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#!unzip language-translation-englishfrench.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfvojjHQlrsU",
        "outputId": "411f9b54-1d2a-484e-cc4c-8a40b12e4dd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-12-08 00:24:35--  https://raw.githubusercontent.com/yandexdataschool/nlp_course/2020/week04_seq2seq/vocab.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2879 (2.8K) [text/plain]\n",
            "Saving to: ‘vocab.py’\n",
            "\n",
            "\rvocab.py              0%[                    ]       0  --.-KB/s               \rvocab.py            100%[===================>]   2.81K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-12-08 00:24:35 (33.6 MB/s) - ‘vocab.py’ saved [2879/2879]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# !pip3 install torch>=1.3.0\n",
        "!pip3 install subword-nmt &> log\n",
        "#!wget https://www.dropbox.com/s/yy2zqh34dyhv07i/data.txt?dl=1 -O data.txt\n",
        "!wget https://raw.githubusercontent.com/yandexdataschool/nlp_course/2020/week04_seq2seq/vocab.py -O vocab.py\n",
        "# thanks to tilda and deephack teams for the data, Dmitry Emelyanenko for the code :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftz2jyA4sjnV",
        "outputId": "53de56f9-c4cb-435d-bd46-ecbcce04c21e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8000/8000 [00:07<00:00, 1036.36it/s]\n",
            "100%|██████████| 8000/8000 [00:11<00:00, 715.01it/s]\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from subword_nmt.learn_bpe import learn_bpe\n",
        "from subword_nmt.apply_bpe import BPE\n",
        "tokenizer = WordPunctTokenizer()\n",
        "def tokenize(x):\n",
        "    return ' '.join(tokenizer.tokenize(x.lower()))\n",
        "\n",
        "# split and tokenize the data\n",
        "with open('train.en', 'w') as f_src,  open('train.fr', 'w') as f_dst:\n",
        "  with open('eng_-french.csv', 'r') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    header = next(csv_reader)\n",
        "    for line in csv_reader:\n",
        "        src_line, dst_line = line[0], line[1]\n",
        "        f_src.write(tokenize(src_line) + '\\n')\n",
        "        f_dst.write(tokenize(dst_line) + '\\n')\n",
        "\n",
        "# build and apply bpe vocs\n",
        "bpe = {}\n",
        "for lang in ['en', 'fr']:\n",
        "    learn_bpe(open('./train.' + lang), open('bpe_rules.' + lang, 'w'), num_symbols=8000)\n",
        "    bpe[lang] = BPE(open('./bpe_rules.' + lang))\n",
        "\n",
        "    with open('train.bpe.' + lang, 'w') as f_out:\n",
        "        for line in open('train.' + lang):\n",
        "            f_out.write(bpe[lang].process_line(line.strip()) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8aRmTrJEvlPj",
        "outputId": "5c86ff64-a18c-4d41-9778-91fad1bd2008"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'A quick brown fox ju@@ mps over a lazy dog'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bpe['en'].process_line('A quick brown fox jumps over a lazy dog')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UPW3sV8lrsb"
      },
      "source": [
        "### Building vocabularies\n",
        "\n",
        "We now need to build vocabularies that map strings to token ids and vice versa. We're gonna need these fellas when we feed training data into model or convert output matrices into words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmTy_m_olrsb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PskgBSxlrsd",
        "outputId": "3667ee49-869d-4d52-b2cf-31bedb93e935"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inp: chez quel gla@@ cier allez - vous ?\n",
            "out: which ice cream shop are you going to ?\n",
            "\n",
            "inp: il fallait s ' y attendre .\n",
            "out: it was to be expected .\n",
            "\n",
            "inp: soyez dis@@ cr@@ ète !\n",
            "out: be discreet .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#data_inp = np.array(open('./train.bpe.ru').read().split('\\n'))\n",
        "data_inp = np.array(open('./train.bpe.fr').read().split('\\n'))\n",
        "data_out = np.array(open('./train.bpe.en').read().split('\\n'))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_inp, dev_inp, train_out, dev_out = train_test_split(data_inp, data_out, test_size=3000,\n",
        "                                                          random_state=42)\n",
        "for i in range(3):\n",
        "    print('inp:', train_inp[i])\n",
        "    print('out:', train_out[i], end='\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vipg4O61lrsg"
      },
      "outputs": [],
      "source": [
        "from vocab import Vocab\n",
        "inp_voc = Vocab.from_lines(train_inp)\n",
        "out_voc = Vocab.from_lines(train_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHWgx34flrsn"
      },
      "source": [
        "### Encoder-decoder model\n",
        "\n",
        "The code below contains a template for a simple encoder-decoder model: single GRU encoder/decoder, no attention or anything. This model is implemented for you as a reference and a baseline for your homework assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd_rDRm9lrso"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wsa-C5zzvlPo"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # f'cuda:{2}' if torch.cuda.is_available() else 'cpu' #0 -is GPU2, 1 is GPU3, 3 is GPU1, 4 is 4 5 is GPU5,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgfN5-F7lrst"
      },
      "outputs": [],
      "source": [
        "class BasicModel(nn.Module):\n",
        "    def __init__(self, inp_voc, out_voc, emb_size=64, hid_size=128):\n",
        "        \"\"\"\n",
        "        A simple encoder-decoder seq2seq model\n",
        "        \"\"\"\n",
        "        super().__init__() # initialize base class to track sub-layers, parameters, etc.\n",
        "\n",
        "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
        "        self.hid_size = hid_size\n",
        "\n",
        "        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)\n",
        "        self.emb_out = nn.Embedding(len(out_voc), emb_size)\n",
        "        self.enc0 = nn.GRU(emb_size, hid_size, batch_first=True)\n",
        "\n",
        "        self.dec_start = nn.Linear(hid_size, hid_size) #connection between encoder and decoder\n",
        "        self.dec0 = nn.GRUCell(emb_size, hid_size)\n",
        "        self.logits = nn.Linear(hid_size, len(out_voc))\n",
        "\n",
        "    def forward(self, inp, out):\n",
        "        \"\"\" Apply model in training mode \"\"\"\n",
        "        initial_state = self.encode(inp)\n",
        "        return self.decode(initial_state, out)\n",
        "\n",
        "\n",
        "    def encode(self, inp, **flags):\n",
        "        \"\"\"\n",
        "        Takes symbolic input sequence, computes initial state\n",
        "        :param inp: matrix of input tokens [batch, time]\n",
        "        :returns: initial decoder state tensors, one or many\n",
        "        \"\"\"\n",
        "        inp_emb = self.emb_inp(inp)\n",
        "        batch_size = inp.shape[0]\n",
        "\n",
        "        enc_seq, [last_state_but_not_really] = self.enc0(inp_emb)\n",
        "        # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]\n",
        "\n",
        "        # note: last_state is not _actually_ last because of padding, let's find the real last_state\n",
        "        lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)\n",
        "        # print((inp != self.inp_voc.eos_ix).to(torch.int64))\n",
        "        last_state = enc_seq[torch.arange(len(enc_seq)), lengths]\n",
        "        # ^-- shape: [batch_size, hid_size]\n",
        "\n",
        "        dec_start = self.dec_start(last_state)\n",
        "        return [dec_start]\n",
        "\n",
        "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
        "        \"\"\"\n",
        "        Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
        "        :param prev_state: a list of previous decoder state tensors, same as returned by encode(...)\n",
        "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
        "        :return: a list of next decoder state tensors, a tensor of logits [batch, len(out_voc)]\n",
        "        \"\"\"\n",
        "        # prev_gru0_state = prev_state[0]\n",
        "        [prev_gru0_state, ] = prev_state\n",
        "\n",
        "        prev_token_embs = self.emb_out(prev_tokens)\n",
        "\n",
        "        new_gru_activations = self.dec0(prev_token_embs, prev_gru0_state)\n",
        "        new_dec_state = [new_gru_activations]\n",
        "        output_logits = self.logits(new_gru_activations)\n",
        "\n",
        "        return new_dec_state, output_logits\n",
        "\n",
        "    def decode(self, initial_state, out_tokens, **flags):\n",
        "        \"\"\" Iterate over reference tokens (out_tokens) with decode_step \"\"\"\n",
        "        batch_size = out_tokens.shape[0]\n",
        "        state = initial_state\n",
        "\n",
        "        # initial logits: always predict BOS\n",
        "        onehot_bos = F.one_hot(torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64),\n",
        "                               num_classes=len(self.out_voc)).to(device=out_tokens.device)\n",
        "        first_logits = torch.log(onehot_bos.to(torch.float32) + 1e-9)\n",
        "\n",
        "        logits_sequence = [first_logits]\n",
        "        for i in range(out_tokens.shape[1] - 1):\n",
        "            state, logits = self.decode_step(state, out_tokens[:, i])\n",
        "            logits_sequence.append(logits)\n",
        "        return torch.stack(logits_sequence, dim=1)\n",
        "\n",
        "    def decode_inference(self, initial_state, max_len=100, **flags):\n",
        "        \"\"\" Generate translations from model (greedy version) \"\"\"\n",
        "        batch_size, device = len(initial_state[0]), initial_state[0].device\n",
        "        state = initial_state\n",
        "        outputs = [torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64,\n",
        "                              device=device)]\n",
        "        all_states = [initial_state]\n",
        "\n",
        "        for i in range(max_len):\n",
        "            state, logits = self.decode_step(state, outputs[-1])\n",
        "            outputs.append(logits.argmax(dim=-1))\n",
        "            all_states.append(state)\n",
        "\n",
        "        return torch.stack(outputs, dim=1), all_states\n",
        "\n",
        "    def translate_lines(self, inp_lines, **kwargs):\n",
        "        inp = self.inp_voc.to_matrix(inp_lines).to(device)\n",
        "        initial_state = self.encode(inp)\n",
        "        out_ids, states = self.decode_inference(initial_state, **kwargs)\n",
        "        return self.out_voc.to_lines(out_ids.cpu().numpy()), states\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wuv1-aVlrs0"
      },
      "source": [
        "### Training loss\n",
        "\n",
        "Our training objective is almost the same as it was for neural language models:\n",
        "$$ L = {\\frac1{|D|}} \\sum_{X, Y \\in D} \\sum_{y_t \\in Y} - \\log p(y_t \\mid y_1, \\dots, y_{t-1}, X, \\theta) $$\n",
        "\n",
        "where $|D|$ is the __total length of all sequences__, including BOS and first EOS, but excluding PAD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpbaBpW7lrs-"
      },
      "source": [
        "### Evaluation: BLEU\n",
        "\n",
        "Machine translation is commonly evaluated with [BLEU](https://en.wikipedia.org/wiki/BLEU) score. This metric simply computes which fraction of predicted n-grams is actually present in the reference translation. It does so for n=1,2,3 and 4 and computes the geometric average with penalty if translation is shorter than reference.\n",
        "\n",
        "While BLEU [has many drawbacks](http://www.cs.jhu.edu/~ccb/publications/re-evaluating-the-role-of-bleu-in-mt-research.pdf), it still remains the most commonly used metric and one of the simplest to compute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edk_oVg0lrtW"
      },
      "source": [
        "### Your Attention Required\n",
        "\n",
        "In this section we want you to improve over the basic model by implementing a simple attention mechanism.\n",
        "\n",
        "This is gonna be a two-parter: building the __attention layer__ and using it for an __attentive seq2seq model__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz9aROAIlrtX"
      },
      "source": [
        "### Attention layer (1 points)\n",
        "\n",
        "Here you will have to implement a layer that computes a simple additive attention:\n",
        "\n",
        "Given encoder sequence $ h^e_0, h^e_1, h^e_2, ..., h^e_T$ and a single decoder state $h^d$,\n",
        "\n",
        "* Compute logits with a 2-layer neural network\n",
        "$$a_t = linear_{out}(tanh(linear_{e}(h^e_t) + linear_{d}(h_d)))$$\n",
        "* Get probabilities from logits,\n",
        "$$ p_t = {{e ^ {a_t}} \\over { \\sum_\\tau e^{a_\\tau} }} $$\n",
        "\n",
        "* Add up encoder states with probabilities to get __attention response__\n",
        "$$ attn = \\sum_t p_t \\cdot h^e_t $$\n",
        "\n",
        "You can learn more about attention layers in the lecture slides or [from this post](https://distill.pub/2016/augmented-rnns/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFNhNqajvlP0"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, name, enc_size, dec_size, hid_size, activ=torch.tanh):\n",
        "        \"\"\" A layer that computes additive attention response and weights \"\"\"\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.enc_size = enc_size # num units in encoder state\n",
        "        self.dec_size = dec_size # num units in decoder state\n",
        "        self.hid_size = hid_size # attention layer hidden units\n",
        "        self.activ = activ       # attention layer hidden nonlinearity\n",
        "\n",
        "        # create trainable paramteres like this:\n",
        "        self.<PARAMETER_NAME> = nn.Parameter(<INITIAL_VALUES>, requires_grad=True)\n",
        "        <...>  # you will need a couple of these\n",
        "\n",
        "\n",
        "    def forward(self, enc, dec, inp_mask):\n",
        "        \"\"\"\n",
        "        Computes attention response and weights\n",
        "        :param enc: encoder activation sequence, float32[batch_size, ninp, enc_size]\n",
        "        :param dec: single decoder state used as \"query\", float32[batch_size, dec_size]\n",
        "        :param inp_mask: mask on enc activatons (0 after first eos), float32 [batch_size, ninp]\n",
        "        :returns: attn[batch_size, enc_size], probs[batch_size, ninp]\n",
        "            - attn - attention response vector (weighted sum of enc)\n",
        "            - probs - attention weights after softmax\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute logits\n",
        "        <...>\n",
        "\n",
        "        # Apply mask - if mask is 0, logits should be -inf or -1e9\n",
        "        # You may need torch.where\n",
        "        <...>\n",
        "\n",
        "        # Compute attention probabilities (softmax)\n",
        "        probs = <...>\n",
        "\n",
        "        # Compute attention response using enc and probs\n",
        "        attn = <...>\n",
        "\n",
        "        return attn, probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IalfpdAelrtb"
      },
      "source": [
        "### Seq2seq model with attention\n",
        "\n",
        "You can now use the attention layer to build a network. The simplest way to implement attention is to use it in decoder phase:\n",
        "![img](https://i.imgur.com/6fKHlHb.png)\n",
        "_image from distill.pub [article](https://distill.pub/2016/augmented-rnns/)_\n",
        "\n",
        "On every step, use __previous__ decoder state to obtain attention response. Then feed concat this response to the inputs of next attention layer.\n",
        "\n",
        "The key implementation detail here is __model state__. Put simply, you can add any tensor into the list of `encode` outputs. You will then have access to them at each `decode` step. This may include:\n",
        "* Last RNN hidden states (as in basic model)\n",
        "* The whole sequence of encoder outputs (to attend to) and mask\n",
        "* Attention probabilities (to visualize)\n",
        "\n",
        "_There are, of course, alternative ways to wire attention into your network and different kinds of attention. Take a look at [this](https://arxiv.org/abs/1609.08144), [this](https://arxiv.org/abs/1706.03762) and [this](https://arxiv.org/abs/1808.03867) for ideas. And for image captioning/im2latex there's [visual attention](https://arxiv.org/abs/1502.03044)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrgGy80lQjrX"
      },
      "outputs": [],
      "source": [
        "class AttentiveModel(BasicModel):\n",
        "    def __init__(self, name, inp_voc, out_voc,\n",
        "                 emb_size=64, hid_size=128, attn_size=128):\n",
        "        \"\"\" Translation model that uses attention. See instructions above. \"\"\"\n",
        "        nn.Module.__init__(self)  # initialize base class to track sub-layers, trainable variables, etc.\n",
        "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
        "        self.hid_size = hid_size\n",
        "\n",
        "        <YOUR CODE: initialize layers>\n",
        "\n",
        "    def encode(self, inp, **flags):\n",
        "        \"\"\"\n",
        "        Takes symbolic input sequence, computes initial state\n",
        "        :param inp: matrix of input tokens [batch, time]\n",
        "        :return: a list of initial decoder state tensors\n",
        "        \"\"\"\n",
        "\n",
        "        # encode input sequence, create initial decoder states\n",
        "        <YOUR CODE>\n",
        "\n",
        "        # apply attention layer from initial decoder hidden state\n",
        "        first_attn_probas = <...>\n",
        "\n",
        "        # Build first state: include\n",
        "        # * initial states for decoder recurrent layers\n",
        "        # * encoder sequence and encoder attn mask (for attention)\n",
        "        # * make sure that last state item is attention probabilities tensor\n",
        "\n",
        "        first_state = [<...>, first_attn_probas]\n",
        "        return first_state\n",
        "\n",
        "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
        "        \"\"\"\n",
        "        Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
        "        :param prev_state: a list of previous decoder state tensors\n",
        "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
        "        :return: a list of next decoder state tensors, a tensor of logits [batch, n_tokens]\n",
        "        \"\"\"\n",
        "\n",
        "        <YOUR CODE HERE>\n",
        "        return [new_dec_state, output_logits]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryZCOTEslrtf"
      },
      "source": [
        "### Training attentive model\n",
        "\n",
        "Please reuse the infrastructure you've built for the regular model. I hope you didn't hard-code anything :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVyI5ZokvlP5"
      },
      "outputs": [],
      "source": [
        "<YOUR CODE: create AttentiveModel and training utilities>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ_cZXzRvlP6"
      },
      "outputs": [],
      "source": [
        "<YOUR CODE: training loop>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2-Xj_AkXpif"
      },
      "outputs": [],
      "source": [
        "<YOUR CODE: measure final BLEU>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp-dJDF5Srnv"
      },
      "outputs": [],
      "source": [
        "#torch.save(model.state_dict(), 'attention.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dAyywDEvlP-"
      },
      "source": [
        "### Visualizing model attention (1 points)\n",
        "\n",
        "After training the attentive translation model, you can check it's sanity by visualizing its attention weights.\n",
        "\n",
        "We provided you with a function that draws attention maps using [`Bokeh`](https://bokeh.pydata.org/en/latest/index.html). Once you managed to produce something better than random noise, please leave them in the notebook or save  bokeh figures and add to your sumbission. You can save bokeh images as screenshots or using this button:\n",
        "\n",
        "![bokeh_panel](https://github.com/yandexdataschool/nlp_course/raw/2019/resources/bokeh_panel.png)\n",
        "\n",
        "__Note:__ you're not locked into using bokeh. If you prefer a different visualization method, feel free to use that instead of bokeh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP7GJaLMvlP_"
      },
      "outputs": [],
      "source": [
        "import bokeh.plotting as pl\n",
        "import bokeh.models as bm\n",
        "from bokeh.io import output_notebook, show\n",
        "output_notebook()\n",
        "\n",
        "def draw_attention(inp_line, translation, probs):\n",
        "    \"\"\" An intentionally ambiguous function to visualize attention weights \"\"\"\n",
        "    inp_tokens = inp_voc.tokenize(inp_line)\n",
        "    trans_tokens = out_voc.tokenize(translation)\n",
        "    probs = probs[:len(trans_tokens), :len(inp_tokens)]\n",
        "\n",
        "    fig = pl.figure(x_range=(0, len(inp_tokens)), y_range=(0, len(trans_tokens)),\n",
        "                    x_axis_type=None, y_axis_type=None, tools=[])\n",
        "    fig.image([probs[::-1]], 0, 0, len(inp_tokens), len(trans_tokens))\n",
        "\n",
        "    fig.add_layout(bm.LinearAxis(axis_label='source tokens'), 'above')\n",
        "    fig.xaxis.ticker = np.arange(len(inp_tokens)) + 0.5\n",
        "    fig.xaxis.major_label_overrides = dict(zip(np.arange(len(inp_tokens)) + 0.5, inp_tokens))\n",
        "    fig.xaxis.major_label_orientation = 45\n",
        "\n",
        "    fig.add_layout(bm.LinearAxis(axis_label='translation tokens'), 'left')\n",
        "    fig.yaxis.ticker = np.arange(len(trans_tokens)) + 0.5\n",
        "    fig.yaxis.major_label_overrides = dict(zip(np.arange(len(trans_tokens)) + 0.5, trans_tokens[::-1]))\n",
        "\n",
        "    show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqpCWAY0T0_d",
        "outputId": "3628d556-7bca-497d-f931-826dcd59b91e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prends place !\n",
            "take place .\n",
            "\n",
            "elle ne fait pas la différence entre le bien et le mal .\n",
            "she doesn ' t do the difference between the well and hurt .\n",
            "\n",
            "ne discu@@ te pas au sujet de ma famille !\n",
            "don ' t talk about my family .\n",
            "\n",
            "au lieu de prendre des notes , j ' ai passé tout le cours à gri@@ bou@@ iller .\n",
            "instead of taking notes , i spent everything about it .\n",
            "\n",
            "dis à tom que je suis en colère .\n",
            "tell tom i ' m angry .\n",
            "\n",
            "laissez - moi gérer ça !\n",
            "let me handle this .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# for inp_line, trans_line in zip(dev_inp[::500], model.translate_lines(dev_inp[::500])[0]):\n",
        "#     print(inp_line)\n",
        "#     print(trans_line)\n",
        "#     print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qouLBt4vlP_"
      },
      "outputs": [],
      "source": [
        "inp = dev_inp[::500]\n",
        "\n",
        "trans, states = model.translate_lines(inp)\n",
        "\n",
        "# select attention probs from model state (you may need to change this for your custom model)\n",
        "# attention_probs below must have shape [batch_size, translation_length, input_length], extracted from states\n",
        "# e.g. if attention probs are at the end of each state, use np.stack([state[-1] for state in states], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGlTTt15UbFR"
      },
      "outputs": [],
      "source": [
        "probs = [states[i][-1] for i in range(len(states))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXrHr82NRis9"
      },
      "outputs": [],
      "source": [
        "attention_probs = np.stack([state[-1].cpu().detach().numpy() for state in states], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xb7jHRxWvlQA",
        "outputId": "77196ef0-6cbc-41b6-d2c0-6a7f9734a176"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(null);\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.3.1.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(null)).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
            "application/vnd.bokehjs_load.v0+json": ""
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"e6f2dca8-3cde-4523-8de5-542d775b13b5\" data-root-id=\"p1206\" style=\"display: contents;\"></div>\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"40e10f4d-0268-47a7-8844-f9008b310ff9\":{\"version\":\"3.3.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1206\",\"attributes\":{\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1215\",\"attributes\":{\"end\":5}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1216\",\"attributes\":{\"end\":5}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1217\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1218\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1213\"},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1231\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1219\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1220\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1221\"},\"data\":{\"type\":\"map\",\"entries\":[[\"image\",[{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"B3hxOYTRczsM3648HYZ5PwAAAAAFZBY8re6EPata1z5EWwE/AAAAAHOaVDyLORw+bjdHP0x8Zj0AAAAAS/GRO1xiPz+MzG4+badxPAAAAACK9NQ5hcTcPjD+Bz/uThg9AAAAAA==\"},\"shape\":[5,5],\"dtype\":\"float32\",\"order\":\"little\"}]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1232\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1233\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Image\",\"id\":\"p1222\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":0},\"dw\":{\"type\":\"value\",\"value\":5},\"dh\":{\"type\":\"value\",\"value\":5},\"image\":{\"type\":\"field\",\"field\":\"image\"},\"color_mapper\":{\"type\":\"object\",\"name\":\"LinearColorMapper\",\"id\":\"p1223\",\"attributes\":{\"palette\":[\"#000000\",\"#252525\",\"#525252\",\"#737373\",\"#969696\",\"#bdbdbd\",\"#d9d9d9\",\"#f0f0f0\",\"#ffffff\"]}}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Image\",\"id\":\"p1225\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":0},\"dw\":{\"type\":\"value\",\"value\":5},\"dh\":{\"type\":\"value\",\"value\":5},\"global_alpha\":{\"type\":\"value\",\"value\":0.1},\"image\":{\"type\":\"field\",\"field\":\"image\"},\"color_mapper\":{\"type\":\"object\",\"name\":\"LinearColorMapper\",\"id\":\"p1226\",\"attributes\":{\"palette\":[\"#000000\",\"#252525\",\"#525252\",\"#737373\",\"#969696\",\"#bdbdbd\",\"#d9d9d9\",\"#f0f0f0\",\"#ffffff\"]}}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Image\",\"id\":\"p1228\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":0},\"dw\":{\"type\":\"value\",\"value\":5},\"dh\":{\"type\":\"value\",\"value\":5},\"global_alpha\":{\"type\":\"value\",\"value\":0.2},\"image\":{\"type\":\"field\",\"field\":\"image\"},\"color_mapper\":{\"type\":\"object\",\"name\":\"LinearColorMapper\",\"id\":\"p1229\",\"attributes\":{\"palette\":[\"#000000\",\"#252525\",\"#525252\",\"#737373\",\"#969696\",\"#bdbdbd\",\"#d9d9d9\",\"#f0f0f0\",\"#ffffff\"]}}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1214\"},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1239\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"FixedTicker\",\"id\":\"p1243\",\"attributes\":{\"ticks\":{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAAA4D8AAAAAAAD4PwAAAAAAAARAAAAAAAAADEAAAAAAAAASQA==\"},\"shape\":[5],\"dtype\":\"float64\",\"order\":\"little\"},\"minor_ticks\":[]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1241\"},\"axis_label\":\"translation tokens\",\"major_label_overrides\":{\"type\":\"map\",\"entries\":[[0.5,\"_EOS_\"],[1.5,\".\"],[2.5,\"place\"],[3.5,\"take\"],[4.5,\"_BOS_\"]]},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1242\"}}}],\"above\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1234\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"FixedTicker\",\"id\":\"p1238\",\"attributes\":{\"ticks\":{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAAA4D8AAAAAAAD4PwAAAAAAAARAAAAAAAAADEAAAAAAAAASQA==\"},\"shape\":[5],\"dtype\":\"float64\",\"order\":\"little\"},\"minor_ticks\":[]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1236\"},\"axis_label\":\"source tokens\",\"major_label_orientation\":45,\"major_label_overrides\":{\"type\":\"map\",\"entries\":[[0.5,\"_BOS_\"],[1.5,\"prends\"],[2.5,\"place\"],[3.5,\"!\"],[4.5,\"_EOS_\"]]},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1237\"}}}]}}]}};\n  const render_items = [{\"docid\":\"40e10f4d-0268-47a7-8844-f9008b310ff9\",\"roots\":{\"p1206\":\"e6f2dca8-3cde-4523-8de5-542d775b13b5\"},\"root_ids\":[\"p1206\"]}];\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
            "application/vnd.bokehjs_exec.v0+json": ""
          },
          "metadata": {
            "application/vnd.bokehjs_exec.v0+json": {
              "id": "p1206"
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"d7ecb93f-99b3-47ec-84dd-cf8e623f58ba\" data-root-id=\"p1247\" style=\"display: contents;\"></div>\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"9652b017-54e1-48a3-a4b1-72d2e9162c04\":{\"version\":\"3.3.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1247\",\"attributes\":{\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1256\",\"attributes\":{\"end\":15}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1257\",\"attributes\":{\"end\":15}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1258\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1259\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1254\"},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1272\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1260\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1261\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1262\"},\"data\":{\"type\":\"map\",\"entries\":[[\"image\",[{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"6gbSNziDljkGsy85VV6LOWGM5DjVlmM6ZJEPPZ/p/jtuFWY6AsW1PZIxMz58LhY94HuEPp9xyj4AAAAAaxsQNjwxQjnksZQ5VFooO0FOmDkbBJ04qcavO/c19Tp54rk772abPf5s1T5bshU+ffadPU4IiT4AAAAAzcGgNbXCFzefSlQ4ymumOORX2TjWlbE3tWG1Oe24FjnrpEk5enCSPDNTPj0N1qY8dghWP5JQoD0AAAAAa8aGNq+1czef3hI4357yOe2XvziuF4859IwyPJvCoTwnWig7Mb3kO5qPPD9vLvo7UJDpPMWuPj4AAAAAsynjM92aDTZopik1rav+NQnGwjXYBe02L3dTPAO4VDqDABs774ZqP3anGTx2lMg5JVlpPVvGiToAAAAAArJDNRU+izrhdtg5aG4DOhml4DhhsNg5f4AfPHd0fTy/S5g7jsAgPzRMMT4uRAU86OcPPqOnkjwAAAAAJNpyNqQ+Czrc5sI6P5feOtBspDkzUtk6UCPrOzeqGj8lgo48dHbMPBUenT4ArDw7xV6wPJMDGTwAAAAAuI+TNAQkxDjVVOg2mzFRN9nPMTZSjlk6+SgeP0U2ljyyk/07ttKzPnYw4TqGD3E44EYMO6YMDTgAAAAA37mZN9hvlDo3rHM8O5ZiPc0uTjxIuPk86FscP91uaT7ebk48Hr2nPLPSIjyJqUc6TzjmOoHarDoAAAAAfC5bOeYomTmaZzM50/REPyhY7jywuBo8YBcfPgaExDuOYy06hUcQPO6SBDzhmqk68SWkOxJfyzsAAAAAxZKxOPpgNjr0NEE8NQxxPjhEMD+5qbo7tHDzPI38NTudnZM6HgnZO6p1tjse4Os6PpPmO8AKTjsAAAAA/+MQOWzAGzyG9xA9KwOdPSXpTz/yXd87qh6zPI2noTwU+aY6/QlxO1H68jtAmrw5qLyFO+rfcToAAAAAXSA6Ot4Hpzsxvx89YHfHPghMCz8J3aQ6eLMWPE1kLDuxzgs6hEljOtVXsDtyS3Y5bApuOnAB9jkAAAAAZhLyOo+/Vz8izK89fyncPFblejyDkl87WwIaPJH6fTvzwIw6hjIHOyLqWzv9yaM60P6JOqES9joAAAAAE80WOe8pIDx4IDs8sEbEPRwjwT0yyys7EE4MPr4CJT5WJCs8V3DVPeRpQj4gLzg8XaCQPaGSzz0AAAAA\"},\"shape\":[15,15],\"dtype\":\"float32\",\"order\":\"little\"}]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1273\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1274\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Image\",\"id\":\"p1263\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":0},\"dw\":{\"type\":\"value\",\"value\":15},\"dh\":{\"type\":\"value\",\"value\":15},\"image\":{\"type\":\"field\",\"field\":\"image\"},\"color_mapper\":{\"type\":\"object\",\"name\":\"LinearColorMapper\",\"id\":\"p1264\",\"attributes\":{\"palette\":[\"#000000\",\"#252525\",\"#525252\",\"#737373\",\"#969696\",\"#bdbdbd\",\"#d9d9d9\",\"#f0f0f0\",\"#ffffff\"]}}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Image\",\"id\":\"p1266\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":0},\"dw\":{\"type\":\"value\",\"value\":15},\"dh\":{\"type\":\"value\",\"value\":15},\"global_alpha\":{\"type\":\"value\",\"value\":0.1},\"image\":{\"type\":\"field\",\"field\":\"image\"},\"color_mapper\":{\"type\":\"object\",\"name\":\"LinearColorMapper\",\"id\":\"p1267\",\"attributes\":{\"palette\":[\"#000000\",\"#252525\",\"#525252\",\"#737373\",\"#969696\",\"#bdbdbd\",\"#d9d9d9\",\"#f0f0f0\",\"#ffffff\"]}}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Image\",\"id\":\"p1269\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":0},\"dw\":{\"type\":\"value\",\"value\":15},\"dh\":{\"type\":\"value\",\"value\":15},\"global_alpha\":{\"type\":\"value\",\"value\":0.2},\"image\":{\"type\":\"field\",\"field\":\"image\"},\"color_mapper\":{\"type\":\"object\",\"name\":\"LinearColorMapper\",\"id\":\"p1270\",\"attributes\":{\"palette\":[\"#000000\",\"#252525\",\"#525252\",\"#737373\",\"#969696\",\"#bdbdbd\",\"#d9d9d9\",\"#f0f0f0\",\"#ffffff\"]}}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1255\"},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1280\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"FixedTicker\",\"id\":\"p1284\",\"attributes\":{\"ticks\":{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAAA4D8AAAAAAAD4PwAAAAAAAARAAAAAAAAADEAAAAAAAAASQAAAAAAAABZAAAAAAAAAGkAAAAAAAAAeQAAAAAAAACFAAAAAAAAAI0AAAAAAAAAlQAAAAAAAACdAAAAAAAAAKUAAAAAAAAArQAAAAAAAAC1A\"},\"shape\":[15],\"dtype\":\"float64\",\"order\":\"little\"},\"minor_ticks\":[]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1282\"},\"axis_label\":\"translation tokens\",\"major_label_overrides\":{\"type\":\"map\",\"entries\":[[0.5,\"_EOS_\"],[1.5,\".\"],[2.5,\"hurt\"],[3.5,\"and\"],[4.5,\"well\"],[5.5,\"the\"],[6.5,\"between\"],[7.5,\"difference\"],[8.5,\"the\"],[9.5,\"do\"],[10.5,\"t\"],[11.5,\"'\"],[12.5,\"doesn\"],[13.5,\"she\"],[14.5,\"_BOS_\"]]},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1283\"}}}],\"above\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1275\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"FixedTicker\",\"id\":\"p1279\",\"attributes\":{\"ticks\":{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAAA4D8AAAAAAAD4PwAAAAAAAARAAAAAAAAADEAAAAAAAAASQAAAAAAAABZAAAAAAAAAGkAAAAAAAAAeQAAAAAAAACFAAAAAAAAAI0AAAAAAAAAlQAAAAAAAACdAAAAAAAAAKUAAAAAAAAArQAAAAAAAAC1A\"},\"shape\":[15],\"dtype\":\"float64\",\"order\":\"little\"},\"minor_ticks\":[]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1277\"},\"axis_label\":\"source tokens\",\"major_label_orientation\":45,\"major_label_overrides\":{\"type\":\"map\",\"entries\":[[0.5,\"_BOS_\"],[1.5,\"elle\"],[2.5,\"ne\"],[3.5,\"fait\"],[4.5,\"pas\"],[5.5,\"la\"],[6.5,\"diff\\u00e9rence\"],[7.5,\"entre\"],[8.5,\"le\"],[9.5,\"bien\"],[10.5,\"et\"],[11.5,\"le\"],[12.5,\"mal\"],[13.5,\".\"],[14.5,\"_EOS_\"]]},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1278\"}}}]}}]}};\n  const render_items = [{\"docid\":\"9652b017-54e1-48a3-a4b1-72d2e9162c04\",\"roots\":{\"p1247\":\"d7ecb93f-99b3-47ec-84dd-cf8e623f58ba\"},\"root_ids\":[\"p1247\"]}];\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
            "application/vnd.bokehjs_exec.v0+json": ""
          },
          "metadata": {
            "application/vnd.bokehjs_exec.v0+json": {
              "id": "p1247"
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"ac812cae-f085-4793-b589-a4dd5ddfa907\" data-root-id=\"p1288\" style=\"display: contents;\"></div>\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"c93cf253-3581-4c4a-a2df-b7aad55438d1\":{\"version\":\"3.3.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1288\",\"attributes\":{\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1297\",\"attributes\":{\"end\":12}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1298\",\"attributes\":{\"end\":10}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1299\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1300\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1295\"},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1313\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1301\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1302\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1303\"},\"data\":{\"type\":\"map\",\"entries\":[[\"image\",[{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"8QgNORPZEjqsV1g7LmenO2vMDzxOBXE9CG1zPa89RD3xHIk83KaFPAJrSD8AAAAAi+NPOJFsnjnvsao8M/dBO/WmYTyZqao9fz7zPWOj9T284IY924auPd1Y+j4AAAAAFjOYMimouDMLSI835V0+NMUmwDIZY8Q2PuaWOD07WjeU1TQ57ul/P9jmajgAAAAAd87gNUHrRjlALs87A5P/OzOX5jrXAe86PwoPPUz1gzzlMDE/7MxtPohb2zsAAAAAooOHNz7mDzs1oko9CCsdPXhN0TuxXgI+J2T+Prd4ED7zQ7Y9H/42PbWnlzsAAAAAWqBgOgaBhjvDNUQ/eT57PEjgOD3vNqA86SoMPntLdzvyvgw76MmfO2UOaDoAAAAA/LSPOCrHVzsfAcY90PhbO6v2UD8feSw8R/5TPUDEbDv4bJw7OB/KO7hOZTsAAAAAVn2hOau+6zzDawM96xJAPFgKST/1Fhg9uGXFPeL/Pzu4ki87SAu1Ooh5qDoAAAAAD9oWO0sVJz8v9Lo9c0EsPLxAVz6qGPM7hxdHPPS3qDqCbZU7PQp0O4hPfjsAAAAAn8UqOIVEljtRRmg/9zggOWMv+ztm7Ew6+pffPBNIXjqOdi87qWMWPWY2QzwAAAAA\"},\"shape\":[10,12],\"dtype\":\"float32\",\"order\":\"little\"}]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1314\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1315\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Image\",\"id\":\"p1304\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":0},\"dw\":{\"type\":\"value\",\"value\":12},\"dh\":{\"type\":\"value\",\"value\":10},\"image\":{\"type\":\"field\",\"field\":\"image\"},\"color_mapper\":{\"type\":\"object\",\"name\":\"LinearColorMapper\",\"id\":\"p1305\",\"attributes\":{\"palette\":[\"#000000\",\"#252525\",\"#525252\",\"#737373\",\"#969696\",\"#bdbdbd\",\"#d9d9d9\",\"#f0f0f0\",\"#ffffff\"]}}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Image\",\"id\":\"p1307\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":0},\"dw\":{\"type\":\"value\",\"value\":12},\"dh\":{\"type\":\"value\",\"value\":10},\"global_alpha\":{\"type\":\"value\",\"value\":0.1},\"image\":{\"type\":\"field\",\"field\":\"image\"},\"color_mapper\":{\"type\":\"object\",\"name\":\"LinearColorMapper\",\"id\":\"p1308\",\"attributes\":{\"palette\":[\"#000000\",\"#252525\",\"#525252\",\"#737373\",\"#969696\",\"#bdbdbd\",\"#d9d9d9\",\"#f0f0f0\",\"#ffffff\"]}}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Image\",\"id\":\"p1310\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":0},\"dw\":{\"type\":\"value\",\"value\":12},\"dh\":{\"type\":\"value\",\"value\":10},\"global_alpha\":{\"type\":\"value\",\"value\":0.2},\"image\":{\"type\":\"field\",\"field\":\"image\"},\"color_mapper\":{\"type\":\"object\",\"name\":\"LinearColorMapper\",\"id\":\"p1311\",\"attributes\":{\"palette\":[\"#000000\",\"#252525\",\"#525252\",\"#737373\",\"#969696\",\"#bdbdbd\",\"#d9d9d9\",\"#f0f0f0\",\"#ffffff\"]}}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1296\"},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1321\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"FixedTicker\",\"id\":\"p1325\",\"attributes\":{\"ticks\":{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAAA4D8AAAAAAAD4PwAAAAAAAARAAAAAAAAADEAAAAAAAAASQAAAAAAAABZAAAAAAAAAGkAAAAAAAAAeQAAAAAAAACFAAAAAAAAAI0A=\"},\"shape\":[10],\"dtype\":\"float64\",\"order\":\"little\"},\"minor_ticks\":[]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1323\"},\"axis_label\":\"translation tokens\",\"major_label_overrides\":{\"type\":\"map\",\"entries\":[[0.5,\"_EOS_\"],[1.5,\".\"],[2.5,\"family\"],[3.5,\"my\"],[4.5,\"about\"],[5.5,\"talk\"],[6.5,\"t\"],[7.5,\"'\"],[8.5,\"don\"],[9.5,\"_BOS_\"]]},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1324\"}}}],\"above\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1316\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"FixedTicker\",\"id\":\"p1320\",\"attributes\":{\"ticks\":{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAAA4D8AAAAAAAD4PwAAAAAAAARAAAAAAAAADEAAAAAAAAASQAAAAAAAABZAAAAAAAAAGkAAAAAAAAAeQAAAAAAAACFAAAAAAAAAI0AAAAAAAAAlQAAAAAAAACdA\"},\"shape\":[12],\"dtype\":\"float64\",\"order\":\"little\"},\"minor_ticks\":[]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1318\"},\"axis_label\":\"source tokens\",\"major_label_orientation\":45,\"major_label_overrides\":{\"type\":\"map\",\"entries\":[[0.5,\"_BOS_\"],[1.5,\"ne\"],[2.5,\"discu@@\"],[3.5,\"te\"],[4.5,\"pas\"],[5.5,\"au\"],[6.5,\"sujet\"],[7.5,\"de\"],[8.5,\"ma\"],[9.5,\"famille\"],[10.5,\"!\"],[11.5,\"_EOS_\"]]},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1319\"}}}]}}]}};\n  const render_items = [{\"docid\":\"c93cf253-3581-4c4a-a2df-b7aad55438d1\",\"roots\":{\"p1288\":\"ac812cae-f085-4793-b589-a4dd5ddfa907\"},\"root_ids\":[\"p1288\"]}];\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
            "application/vnd.bokehjs_exec.v0+json": ""
          },
          "metadata": {
            "application/vnd.bokehjs_exec.v0+json": {
              "id": "p1288"
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"e0c32350-0993-45ca-ab60-3a429260fc20\" data-root-id=\"p1329\" style=\"display: contents;\"></div>\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"36ed2600-5b88-4d39-8a9f-41058f32be6f\":{\"version\":\"3.3.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1329\",\"attributes\":{\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1338\",\"attributes\":{\"end\":21}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1339\",\"attributes\":{\"end\":13}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1340\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1341\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1336\"},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1354\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1342\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1343\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1344\"},\"data\":{\"type\":\"map\",\"entries\":[[\"image\",[{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"k1wTNinTEDe+KPY2npcMOT4+6jeuzTY4KyVqOIAOJTpDh1k8mM0HOytFqjxRbaI92mN+PdFwez1PNrg+LRQQPmw+AD4tkFQ9qM7RPLkhaT0AAAAA9IScNTj6Mzdb90Y3W5aBOFbU3TfNzJE3xkolOcSCPzlEMIk6xVQ2Ogha1DlkbQg8bcVSPev83jy3pRk+in/bPXNSfT5GBjA+brAePhGXoT0AAAAAKGNfNUuLrzaBIhQ43ofrN0xuDzkzbKk22Cr8OfzsyDlcoNw6kwGfOS98ITrLPtc6g0ALPUyEnzyXO/g9CFU8PUsMEz+77gY+q+tfPYPsVzwAAAAAp6eyNKI+fDeg8Ik2tSsvOCHwGjgNuwE4sl8jOoGs9DqAy+I8bK68O3sTATtuG6U8KxCwPaUXqT0dPhM+WssRPhWVpz4WWt49XWnSPJxI0TwAAAAA74JnNIPTXTYrHrE3hUtsN8qheDiDxi83SuRtOj2u8zmAkxg7HCG1OUn5izq8Hws+jhjrPvhURT7x1CI+AOBuPO8o3TzKFV875RUTO/Z1UDoAAAAA66qrNGMnATWji1E2c9UfOGp2vDhEtug37/lTOHjBPzrykow6yPQzO00Cwz28r2M/OJQFPM+T2DkzJZU6Nim3OBazCTok/FY5zrlROLvcTDgAAAAABqDANr7TAjqltsA7mrM9OjFh4TviToA6ZBlUO0O2Jzwp9l0/AguXPO8FdDwEFfc8gHRnO68O9TuMGLM8rqb3OhccYTuss6Y6E4WXOmiL1DkAAAAAe8pPN1+ayTh93eM63pkvOyjMhzwTikE8d/DXPdEfHD7SNas+deYSPUi2FTy7bjI+92BiPQu5Iz0hdgA9BmMhPHz8MzwzVNg6vTsxO9QPdTsAAAAALkEqN7C9eTozaP47oqyQPBCPPz6LZSY9xJ4uP5x3RjzD5+o8WNaVOtdogzp1s7M74eVJO2dGtzuIFeA6BdXHOuN+szqZKg46eD6IOj1zCzoAAAAAObEDNuR88jmjgpY8rBnqOxvgHD/vD6E7xtewPrYGNDtPn4k71iogOcgeMDjkLO45PrAFOoQ7hTn/22I6WOkjORYN2zk7wJo5JjDQOSCT3DkAAAAAy2qvOIkvbDscH3M8U2gPP7/MEj5xbdE9f0vzPXJC3zvgqzo8L4Z9O2fZDjujlJw8TqAjPNNUuTpJ3SQ6SHUUOmgFojk2fGE58POEOdKACDkAAAAAy+1qOxLdhD4ORrw+3lKCPTnqDD7qx4s8yQAFPfw5MDxD22Y9Xx00PKTKDTvupgQ87ZH6OzVTOztlJI07EDO3OyVurzvMl9Q6IUj1Oi2ITzoAAAAAD7XEOFAomjrjMp8+AlnaPCozCD614rA6r9iQPG+Wxj16ZSg88HZgOhMITTxSH549T12DPOM5DDw/NJE9uSYnPG/rhTySJk09Gcc0PaSWwj0AAAAA\"},\"shape\":[13,21],\"dtype\":\"float32\",\"order\":\"little\"}]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1355\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1356\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Image\",\"id\":\"p1345\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":0},\"dw\":{\"type\":\"value\",\"value\":21},\"dh\":{\"type\":\"value\",\"value\":13},\"image\":{\"type\":\"field\",\"field\":\"image\"},\"color_mapper\":{\"type\":\"object\",\"name\":\"LinearColorMapper\",\"id\":\"p1346\",\"attributes\":{\"palette\":[\"#000000\",\"#252525\",\"#525252\",\"#737373\",\"#969696\",\"#bdbdbd\",\"#d9d9d9\",\"#f0f0f0\",\"#ffffff\"]}}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Image\",\"id\":\"p1348\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":0},\"dw\":{\"type\":\"value\",\"value\":21},\"dh\":{\"type\":\"value\",\"value\":13},\"global_alpha\":{\"type\":\"value\",\"value\":0.1},\"image\":{\"type\":\"field\",\"field\":\"image\"},\"color_mapper\":{\"type\":\"object\",\"name\":\"LinearColorMapper\",\"id\":\"p1349\",\"attributes\":{\"palette\":[\"#000000\",\"#252525\",\"#525252\",\"#737373\",\"#969696\",\"#bdbdbd\",\"#d9d9d9\",\"#f0f0f0\",\"#ffffff\"]}}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Image\",\"id\":\"p1351\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":0},\"dw\":{\"type\":\"value\",\"value\":21},\"dh\":{\"type\":\"value\",\"value\":13},\"global_alpha\":{\"type\":\"value\",\"value\":0.2},\"image\":{\"type\":\"field\",\"field\":\"image\"},\"color_mapper\":{\"type\":\"object\",\"name\":\"LinearColorMapper\",\"id\":\"p1352\",\"attributes\":{\"palette\":[\"#000000\",\"#252525\",\"#525252\",\"#737373\",\"#969696\",\"#bdbdbd\",\"#d9d9d9\",\"#f0f0f0\",\"#ffffff\"]}}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1337\"},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1362\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"FixedTicker\",\"id\":\"p1366\",\"attributes\":{\"ticks\":{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAAA4D8AAAAAAAD4PwAAAAAAAARAAAAAAAAADEAAAAAAAAASQAAAAAAAABZAAAAAAAAAGkAAAAAAAAAeQAAAAAAAACFAAAAAAAAAI0AAAAAAAAAlQAAAAAAAACdAAAAAAAAAKUA=\"},\"shape\":[13],\"dtype\":\"float64\",\"order\":\"little\"},\"minor_ticks\":[]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1364\"},\"axis_label\":\"translation tokens\",\"major_label_overrides\":{\"type\":\"map\",\"entries\":[[0.5,\"_EOS_\"],[1.5,\".\"],[2.5,\"it\"],[3.5,\"about\"],[4.5,\"everything\"],[5.5,\"spent\"],[6.5,\"i\"],[7.5,\",\"],[8.5,\"notes\"],[9.5,\"taking\"],[10.5,\"of\"],[11.5,\"instead\"],[12.5,\"_BOS_\"]]},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1365\"}}}],\"above\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1357\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"FixedTicker\",\"id\":\"p1361\",\"attributes\":{\"ticks\":{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAAA4D8AAAAAAAD4PwAAAAAAAARAAAAAAAAADEAAAAAAAAASQAAAAAAAABZAAAAAAAAAGkAAAAAAAAAeQAAAAAAAACFAAAAAAAAAI0AAAAAAAAAlQAAAAAAAACdAAAAAAAAAKUAAAAAAAAArQAAAAAAAAC1AAAAAAAAAL0AAAAAAAIAwQAAAAAAAgDFAAAAAAACAMkAAAAAAAIAzQAAAAAAAgDRA\"},\"shape\":[21],\"dtype\":\"float64\",\"order\":\"little\"},\"minor_ticks\":[]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1359\"},\"axis_label\":\"source tokens\",\"major_label_orientation\":45,\"major_label_overrides\":{\"type\":\"map\",\"entries\":[[0.5,\"_BOS_\"],[1.5,\"au\"],[2.5,\"lieu\"],[3.5,\"de\"],[4.5,\"prendre\"],[5.5,\"des\"],[6.5,\"notes\"],[7.5,\",\"],[8.5,\"j\"],[9.5,\"'\"],[10.5,\"ai\"],[11.5,\"pass\\u00e9\"],[12.5,\"tout\"],[13.5,\"le\"],[14.5,\"cours\"],[15.5,\"\\u00e0\"],[16.5,\"gri@@\"],[17.5,\"bou@@\"],[18.5,\"iller\"],[19.5,\".\"],[20.5,\"_EOS_\"]]},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1360\"}}}]}}]}};\n  const render_items = [{\"docid\":\"36ed2600-5b88-4d39-8a9f-41058f32be6f\",\"roots\":{\"p1329\":\"e0c32350-0993-45ca-ab60-3a429260fc20\"},\"root_ids\":[\"p1329\"]}];\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
            "application/vnd.bokehjs_exec.v0+json": ""
          },
          "metadata": {
            "application/vnd.bokehjs_exec.v0+json": {
              "id": "p1329"
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"d31e5b2a-1aab-45a3-b59a-8981a0e50818\" data-root-id=\"p1370\" style=\"display: contents;\"></div>\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"7ba1ffb7-5558-4ce6-92a6-5b5572434b6d\":{\"version\":\"3.3.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1370\",\"attributes\":{\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1379\",\"attributes\":{\"end\":11}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1380\",\"attributes\":{\"end\":9}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1381\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1382\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1377\"},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1395\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1383\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1384\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1385\"},\"data\":{\"type\":\"map\",\"entries\":[[\"image\",[{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"XibBOAeVOTrN5KY7O3cHOwn8lTrY2Jg9qpCJPtiNRT3DBQI+KtvxPgAAAAC27lo3+ECNOZbZszpDFjg8cBMfO/3I6TwO/gA8/hu4PPYoSD10fmA/AAAAAKdeAzdC8qo4uapVOeVrsjl6Qpw5v13sOtO6GTxpiWQ7DUh5PzJRLjwAAAAAK7WiNp4KgDpUdQo7pIAQOgLBwztYr0w8kwYfPzRULTxEDbA+1U0UOwAAAAAT4Uw2FW97OpNkLjr+Ucw5/P5bOx3aeTwJa+w+6cSaO+vcAj8+2OA6AAAAAF5ojjgvTMg6j0DyOwE4PTzYHJo9SCZJPzAs4jwhs7E7uNusPbnGhzoAAAAAEQ/+OEwW8jutKp088VM3P5b/Zj0coDU+LTcvO0kbKjsi35A8ZY7yOQAAAAAXr786+MovP4rY2jwNdks+1rmRPOzbaz2cP5k7KsUpO2AGWzs/hvA5AAAAANLkQjkyuGM/ZBCDOqBOkzs5l646a3bmPIX01zu/6OU7x8xtPXvzbzsAAAAA\"},\"shape\":[9,11],\"dtype\":\"float32\",\"order\":\"little\"}]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1396\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1397\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Image\",\"id\":\"p1386\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":0},\"dw\":{\"type\":\"value\",\"value\":11},\"dh\":{\"type\":\"value\",\"value\":9},\"image\":{\"type\":\"field\",\"field\":\"image\"},\"color_mapper\":{\"type\":\"object\",\"name\":\"LinearColorMapper\",\"id\":\"p1387\",\"attributes\":{\"palette\":[\"#000000\",\"#252525\",\"#525252\",\"#737373\",\"#969696\",\"#bdbdbd\",\"#d9d9d9\",\"#f0f0f0\",\"#ffffff\"]}}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Image\",\"id\":\"p1389\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":0},\"dw\":{\"type\":\"value\",\"value\":11},\"dh\":{\"type\":\"value\",\"value\":9},\"global_alpha\":{\"type\":\"value\",\"value\":0.1},\"image\":{\"type\":\"field\",\"field\":\"image\"},\"color_mapper\":{\"type\":\"object\",\"name\":\"LinearColorMapper\",\"id\":\"p1390\",\"attributes\":{\"palette\":[\"#000000\",\"#252525\",\"#525252\",\"#737373\",\"#969696\",\"#bdbdbd\",\"#d9d9d9\",\"#f0f0f0\",\"#ffffff\"]}}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Image\",\"id\":\"p1392\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":0},\"dw\":{\"type\":\"value\",\"value\":11},\"dh\":{\"type\":\"value\",\"value\":9},\"global_alpha\":{\"type\":\"value\",\"value\":0.2},\"image\":{\"type\":\"field\",\"field\":\"image\"},\"color_mapper\":{\"type\":\"object\",\"name\":\"LinearColorMapper\",\"id\":\"p1393\",\"attributes\":{\"palette\":[\"#000000\",\"#252525\",\"#525252\",\"#737373\",\"#969696\",\"#bdbdbd\",\"#d9d9d9\",\"#f0f0f0\",\"#ffffff\"]}}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1378\"},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1403\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"FixedTicker\",\"id\":\"p1407\",\"attributes\":{\"ticks\":{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAAA4D8AAAAAAAD4PwAAAAAAAARAAAAAAAAADEAAAAAAAAASQAAAAAAAABZAAAAAAAAAGkAAAAAAAAAeQAAAAAAAACFA\"},\"shape\":[9],\"dtype\":\"float64\",\"order\":\"little\"},\"minor_ticks\":[]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1405\"},\"axis_label\":\"translation tokens\",\"major_label_overrides\":{\"type\":\"map\",\"entries\":[[0.5,\"_EOS_\"],[1.5,\".\"],[2.5,\"angry\"],[3.5,\"m\"],[4.5,\"'\"],[5.5,\"i\"],[6.5,\"tom\"],[7.5,\"tell\"],[8.5,\"_BOS_\"]]},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1406\"}}}],\"above\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1398\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"FixedTicker\",\"id\":\"p1402\",\"attributes\":{\"ticks\":{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAAA4D8AAAAAAAD4PwAAAAAAAARAAAAAAAAADEAAAAAAAAASQAAAAAAAABZAAAAAAAAAGkAAAAAAAAAeQAAAAAAAACFAAAAAAAAAI0AAAAAAAAAlQA==\"},\"shape\":[11],\"dtype\":\"float64\",\"order\":\"little\"},\"minor_ticks\":[]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1400\"},\"axis_label\":\"source tokens\",\"major_label_orientation\":45,\"major_label_overrides\":{\"type\":\"map\",\"entries\":[[0.5,\"_BOS_\"],[1.5,\"dis\"],[2.5,\"\\u00e0\"],[3.5,\"tom\"],[4.5,\"que\"],[5.5,\"je\"],[6.5,\"suis\"],[7.5,\"en\"],[8.5,\"col\\u00e8re\"],[9.5,\".\"],[10.5,\"_EOS_\"]]},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1401\"}}}]}}]}};\n  const render_items = [{\"docid\":\"7ba1ffb7-5558-4ce6-92a6-5b5572434b6d\",\"roots\":{\"p1370\":\"d31e5b2a-1aab-45a3-b59a-8981a0e50818\"},\"root_ids\":[\"p1370\"]}];\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
            "application/vnd.bokehjs_exec.v0+json": ""
          },
          "metadata": {
            "application/vnd.bokehjs_exec.v0+json": {
              "id": "p1370"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "for i in range(5):\n",
        "    draw_attention(inp[i], trans[i], attention_probs[i])\n",
        "\n",
        "# Does it look fine already? don't forget to save images for anytask!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu875YfmvlQA"
      },
      "source": [
        "__Note:__ If the attention maps are not iterpretable, try starting encoder from zeros (instead of dec_start), forcing model to use attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbIIngNVlrtt"
      },
      "source": [
        "## Implement Two different archetectures with attention (8 points)\n",
        "\n",
        "We want you to find the best model for the task. Use everything you know.\n",
        "\n",
        "* add attention after RNN or as a hidden state input\n",
        "* different recurrent units: rnn/gru/lstm; deeper architectures\n",
        "* bidirectional encoder, different attention methods for decoder (additive, dot-product, multi-head)\n",
        "* word dropout, training schedules, anything you can imagine\n",
        "* replace greedy inference with beam search\n",
        "\n",
        "Describe what you tried and what results you obtained in a short report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as7VjVJaOQAJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "edk_oVg0lrtW"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": ".venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}