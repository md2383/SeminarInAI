{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSWEcS2XKgzi"
      },
      "source": [
        "# HW 7 (10 pt)\n",
        "### Parameter Efficient Fine-Tuning\n",
        "In this notebook, you're gonna fine-tune large language models within limited GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T01:32:25.834446Z",
          "iopub.status.busy": "2025-03-16T01:32:25.834037Z",
          "iopub.status.idle": "2025-03-16T01:32:50.029616Z",
          "shell.execute_reply": "2025-03-16T01:32:50.028904Z",
          "shell.execute_reply.started": "2025-03-16T01:32:25.834402Z"
        },
        "id": "7xeRF_hSKgzs",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "m:\\Classes\\AI Seminar\\aiseminar\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.6.0+cu118\n"
          ]
        }
      ],
      "source": [
        "#!pip install --quiet --upgrade transformers bitsandbytes accelerate sentencepiece optimum auto-gptq\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from transformers import BitsAndBytesConfig\n",
        "from tqdm.auto import tqdm, trange\n",
        "assert torch.cuda.is_available(), \"you need cuda for this part\"\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # use 'cuda' for any GPU\n",
        "# print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T01:32:50.031084Z",
          "iopub.status.busy": "2025-03-16T01:32:50.030706Z",
          "iopub.status.idle": "2025-03-16T01:35:09.589061Z",
          "shell.execute_reply": "2025-03-16T01:35:09.588122Z",
          "shell.execute_reply.started": "2025-03-16T01:32:50.031063Z"
        },
        "id": "VMzFwx29Kgzu",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
            "Fetching 33 files: 100%|██████████| 33/33 [12:54<00:00, 23.47s/it]  \n",
            "Loading checkpoint shards: 100%|██████████| 33/33 [00:10<00:00,  3.29it/s]\n"
          ]
        }
      ],
      "source": [
        "model_name = 'Enoch/llama-7b-hf'\n",
        "\n",
        "# loading Llama tokenizer ...\n",
        "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# ... and the model itself\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map=device, low_cpu_mem_usage=True, offload_state_dict=True, # use device_map='auto' for multipl GPUs\n",
        "    quantization_config=quantization_config, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
        ")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "\n",
        "model.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\n",
        "model.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n",
        "# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgspB2JwSIS2"
      },
      "source": [
        "### Prompt tuning: the story of a fox (2 pts)\n",
        "\n",
        "![img](https://i.imgur.com/Ux3qQAu.png) (source: theodd1souts.fandom.com)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-15T21:03:12.608021Z",
          "iopub.status.busy": "2025-03-15T21:03:12.60771Z",
          "iopub.status.idle": "2025-03-15T21:03:16.080822Z",
          "shell.execute_reply": "2025-03-15T21:03:16.080011Z",
          "shell.execute_reply.started": "2025-03-15T21:03:12.607997Z"
        },
        "id": "H13pYFRxQi4U",
        "outputId": "597e1af9-399a-41ab-8d8d-9c1c216d906c",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Output: <s>A quick brown fox jumps over the lazy dog.\n",
            "A quick\n"
          ]
        }
      ],
      "source": [
        "prompt = 'A quick brown fox'\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "\n",
        "for i in range(10):\n",
        "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVhZACT6SgLq"
      },
      "source": [
        "What a blatant lie! This particular fox assures you that it did not, in fact, jump over the lazy dog. No, sir! The fox was merely minding its own business. __Your task is to train the model to tell the truth: no dog was jumped over today.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-15T01:49:11.67137Z",
          "iopub.status.busy": "2025-03-15T01:49:11.671042Z",
          "iopub.status.idle": "2025-03-15T01:49:12.352048Z",
          "shell.execute_reply": "2025-03-15T01:49:12.351244Z",
          "shell.execute_reply.started": "2025-03-15T01:49:11.671349Z"
        },
        "id": "_r6UVDl4NEua",
        "outputId": "67ab27e0-af96-41c7-f0a9-db92e842cd80",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: tensor(3.0729, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "outputs = model(**batch)\n",
        "\n",
        "next_word_logits = outputs.logits[:, :-1]\n",
        "true_next_tokens = batch['input_ids'][:, 1:]\n",
        "loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "\n",
        "print(\"Loss:\", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amvNufS8WXa0"
      },
      "source": [
        "Except, we can't train the entire model - that would be 28GB gradients in float32. Instead, let's run [prompt tuning](https://arxiv.org/abs/2104.08691).\n",
        "\n",
        "![img](https://i.imgur.com/VwNNKnb.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEkoFNdlshv_"
      },
      "source": [
        "### Using HuggingFace PEFT (5 points)\n",
        "\n",
        "[`peft`](https://huggingface.co/docs/peft/index) is a transformer's sister library that allows you to apply various **p**arameter **e**fficient **f**ine-**t**uning methods to pre-trained transformers. The library imlements both prompt tuning, prefix tuning, as well as several adapter-based techniques under a common interface:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-15T02:13:01.295016Z",
          "iopub.status.busy": "2025-03-15T02:13:01.294666Z",
          "iopub.status.idle": "2025-03-15T02:13:01.316097Z",
          "shell.execute_reply": "2025-03-15T02:13:01.315189Z",
          "shell.execute_reply.started": "2025-03-15T02:13:01.29499Z"
        },
        "id": "mqEEpZm2Q4UC",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "2b760d0e-ac1e-4580-9472-997d1275385d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable parameters: 65536\n",
            "Total parameters (excluding quantization): 3506769920\n"
          ]
        }
      ],
      "source": [
        "import peft\n",
        "assert isinstance(model.model.embed_tokens, nn.Embedding), \"please reload the model\"\n",
        "\n",
        "peft_config = peft.PromptTuningConfig(task_type=peft.TaskType.CAUSAL_LM, num_virtual_tokens=16)\n",
        "model = peft.get_peft_model(model, peft_config)  # note: for most peft methods, this line also modifies model in-place\n",
        "print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "print(\"Total parameters (excluding quantization):\", sum(p.numel() for p in model.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-IlPcRhoM8e"
      },
      "source": [
        "# Your task: optimize the PEFT-wrapped model to achieve next token prediction loss < 0.1, but this time using PEFT\n",
        "# Please note: you no longer need to prepend PAD tokens, but you still need to skip :num_virtual_tokens: first logits.\n",
        "# Finally, generate the sentence to make sure that the model learned the truth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfeUMracocqG"
      },
      "source": [
        "You may need to use the following parameters in optimizor:\n",
        "`Adam([param for name, param in model.named_parameters() if param.requires_grad], lr=0.01)`\n",
        "\n",
        "For multiple GPU usage you can use `model = torch.nn.DataParallel(model)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-15T21:03:36.700415Z",
          "iopub.status.busy": "2025-03-15T21:03:36.700032Z",
          "iopub.status.idle": "2025-03-15T21:03:36.706634Z",
          "shell.execute_reply": "2025-03-15T21:03:36.705329Z",
          "shell.execute_reply.started": "2025-03-15T21:03:36.700382Z"
        },
        "id": "mCRGv_u3xZkI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "tokens = tokenizer(the_truth, return_tensors='pt').to(device)\n",
        "in_ids = tokens['input_ids'].squeeze(0) \n",
        "attention_mask = tokens['attention_mask'].squeeze(0)\n",
        "labels = in_ids.clone()\n",
        "\n",
        "traindata = [{\n",
        "    'input_ids': in_ids,\n",
        "    'attention_mask': attention_mask,\n",
        "    'labels': labels,\n",
        "}]\n",
        "traindataloader = torch.utils.data.DataLoader(traindata, batch_size=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-15T21:03:49.971306Z",
          "iopub.status.busy": "2025-03-15T21:03:49.970851Z",
          "iopub.status.idle": "2025-03-15T21:03:49.977123Z",
          "shell.execute_reply": "2025-03-15T21:03:49.97617Z",
          "shell.execute_reply.started": "2025-03-15T21:03:49.971266Z"
        },
        "id": "LiDz6ABmxZkJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.train()  # set the model to training mode\n",
        "\n",
        "adam_optimizer = Adam(\n",
        "    [param for name, param in model.named_parameters() if param.requires_grad],\n",
        "    lr=0.01\n",
        ")\n",
        "\n",
        "num_virtual_tokens = peft_config.num_virtual_tokens\n",
        "target_loss = 0.01\n",
        "n_epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-15T21:11:54.15018Z",
          "iopub.status.busy": "2025-03-15T21:11:54.149796Z",
          "iopub.status.idle": "2025-03-15T21:11:56.140523Z",
          "shell.execute_reply": "2025-03-15T21:11:56.139518Z",
          "shell.execute_reply.started": "2025-03-15T21:11:54.150141Z"
        },
        "id": "T4BPyw0_xZkJ",
        "outputId": "18ff73a0-47ab-43b0-af1b-8c4d9eab82fb",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 2.2268\n",
            "Epoch 1 | Loss: 2.2252\n",
            "Epoch 2 | Loss: 2.2126\n",
            "Epoch 3 | Loss: 2.2151\n",
            "Epoch 4 | Loss: 2.2183\n",
            "Epoch 5 | Loss: 2.2174\n",
            "Epoch 6 | Loss: 2.2174\n",
            "Epoch 7 | Loss: 2.2187\n",
            "Epoch 8 | Loss: 2.2112\n",
            "Epoch 9 | Loss: 2.2085\n",
            "Epoch 10 | Loss: 2.2044\n",
            "Epoch 11 | Loss: 2.2087\n",
            "Epoch 12 | Loss: 2.2067\n",
            "Epoch 13 | Loss: 2.2189\n",
            "Epoch 14 | Loss: 2.2223\n",
            "Epoch 15 | Loss: 2.2179\n",
            "Epoch 16 | Loss: 2.2224\n",
            "Epoch 17 | Loss: 2.2231\n",
            "Epoch 18 | Loss: 2.2191\n",
            "Epoch 19 | Loss: 2.2114\n",
            "Epoch 20 | Loss: 2.2185\n",
            "Epoch 21 | Loss: 2.2193\n",
            "Epoch 22 | Loss: 2.2092\n",
            "Epoch 23 | Loss: 2.2114\n",
            "Epoch 24 | Loss: 2.2068\n",
            "Epoch 25 | Loss: 2.2063\n",
            "Epoch 26 | Loss: 2.2109\n",
            "Epoch 27 | Loss: 2.2005\n",
            "Epoch 28 | Loss: 2.2021\n",
            "Epoch 29 | Loss: 2.1993\n",
            "Epoch 30 | Loss: 2.2041\n",
            "Epoch 31 | Loss: 2.2106\n",
            "Epoch 32 | Loss: 2.2196\n",
            "Epoch 33 | Loss: 2.2179\n",
            "Epoch 34 | Loss: 2.2183\n",
            "Epoch 35 | Loss: 2.2133\n",
            "Epoch 36 | Loss: 2.2060\n",
            "Epoch 37 | Loss: 2.2072\n",
            "Epoch 38 | Loss: 2.2045\n",
            "Epoch 39 | Loss: 2.2036\n",
            "Epoch 40 | Loss: 2.2117\n",
            "Epoch 41 | Loss: 2.2125\n",
            "Epoch 42 | Loss: 2.2226\n",
            "Epoch 43 | Loss: 2.2084\n",
            "Epoch 44 | Loss: 2.2098\n",
            "Epoch 45 | Loss: 2.2026\n",
            "Epoch 46 | Loss: 2.2050\n",
            "Epoch 47 | Loss: 2.1955\n",
            "Epoch 48 | Loss: 2.2024\n",
            "Epoch 49 | Loss: 2.2173\n",
            "Epoch 50 | Loss: 2.2072\n",
            "Epoch 51 | Loss: 2.2127\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[143]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m loss = F.cross_entropy(logits.view(-\u001b[32m1\u001b[39m, logits.size(-\u001b[32m1\u001b[39m)), labels.view(-\u001b[32m1\u001b[39m))\n\u001b[32m     30\u001b[39m adam_optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m#torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\u001b[39;00m\n\u001b[32m     33\u001b[39m adam_optimizer.step()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mm:\\Classes\\AI Seminar\\aiseminar\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mm:\\Classes\\AI Seminar\\aiseminar\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mm:\\Classes\\AI Seminar\\aiseminar\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "for epoch in range(n_epochs):\n",
        "    for batch in traindataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Skip first `num_virtual_tokens` logits and matching labels\n",
        "        # logits = logits[:, num_virtual_tokens:-1, :].contiguous()\n",
        "        # labels = labels[:, num_virtual_tokens + 1:].contiguous()\n",
        "\n",
        "        seq_len = min(\n",
        "            logits.shape[1] - num_virtual_tokens - 1,\n",
        "            labels.shape[1] - num_virtual_tokens - 1\n",
        "        )\n",
        "\n",
        "        logits = logits[:, num_virtual_tokens:num_virtual_tokens + seq_len, :].contiguous()\n",
        "        labels = labels[:, num_virtual_tokens + 1:num_virtual_tokens + 1 + seq_len].contiguous()\n",
        "\n",
        "        # Align lengths to avoid mismatches\n",
        "        # if logits.size(1) != labels.size(1):\n",
        "        #     min_len = min(logits.size(1), labels.size(1))\n",
        "        #     logits = logits[:, :min_len, :]\n",
        "        #     labels = labels[:, :min_len]\n",
        "\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "        adam_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        adam_optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")\n",
        "    if loss.item() < target_loss:\n",
        "        print(\"Loss Target < 0.1 reached\")\n",
        "        print(f\"Final loss: {loss.item():.4f}\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "UW54GnzCwVpp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "m:\\Classes\\AI Seminar\\aiseminar\\Lib\\site-packages\\peft\\peft_model.py:1925: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
            "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text:\n",
            "A quick brown fox it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "prompt = \"A quick brown fox\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=32,\n",
        "    )\n",
        "\n",
        "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "print(\"Generated text:\")\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCkpKYjWxfhk"
      },
      "source": [
        "### Parameter-efficient finetuning with LoRA (5 points)\n",
        "\n",
        "When training on more serious tasks, you can use low-rank adapters based on the [LoRA paper](https://arxiv.org/pdf/2106.09685.pdf).\n",
        "\n",
        "The core idea is to add low-rank adapters __in parallel with existing linear layers,__ like this:\n",
        "<center><img src=\"https://i.imgur.com/6bQLNiG.png\" width=240px></center>\n",
        "\n",
        "In the original LoRA paper, the adapters were only added to attention projection matrices. However, [subsequent works](https://arxiv.org/abs/2305.14314) show that it is useful to adapt FFNs as well. But before we do any training, we need to implement the basic LoRA layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b87c54c3bed847ab933eae8175359169",
            "8aaa22971b3e416eae8394ef3b3b3f0f",
            "e9c4ba0d262c4b76baa00532e209b92f",
            "e6f9064e6ec545debe2e90163f4c712c",
            "6aad5b046def4a7db1048434e874b5d5",
            "dba48e929a2e43ec8e4bb3d4b32475ca",
            "530d66e4732b4d5486165654415bd2dc",
            "2bd4b6acd8004c1e98c064708108938e",
            "09b07105e4f54d2bb5e6cc1c1f1a9c8e",
            "923869a5864c4d3d80fb76c99fff24e2",
            "25e1f3d72230485c9b84cae4f685a69a",
            "a1fc119a899a4e5bbc6650b090a89c39"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T02:13:21.770903Z",
          "iopub.status.busy": "2025-03-16T02:13:21.770498Z",
          "iopub.status.idle": "2025-03-16T02:14:02.279526Z",
          "shell.execute_reply": "2025-03-16T02:14:02.278846Z",
          "shell.execute_reply.started": "2025-03-16T02:13:21.770871Z"
        },
        "id": "8zundaSzx90r",
        "outputId": "3faf7150-7685-4089-cf58-e03e4fce8bc6",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100%|██████████| 33/33 [00:11<00:00,  2.96it/s]\n"
          ]
        }
      ],
      "source": [
        "# re-load the model to remove any previous PEFT tuners\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map=device, low_cpu_mem_usage=True, offload_state_dict=True,\n",
        "    load_in_4bit=True, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
        ")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "MJ_hq4fwyPVR"
      },
      "outputs": [],
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer\"\"\"\n",
        "    def __init__(self, module: nn.Linear, rank: int):\n",
        "        super().__init__()\n",
        "        self.module = module  # pre-trained (frozen) linear layer\n",
        "        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n",
        "        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n",
        "        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Apply self.module and LoRA adapter, return the sum (self.module outputs + adapter outputs)\n",
        "        #  <YOUR CODE HERE>\n",
        "        # return <YOUR CODE>\n",
        "\n",
        "        adpt_out = input @ self.adapter_A @ self.adapter_B\n",
        "        return self.module(input) + adpt_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-15T21:44:06.320027Z",
          "iopub.status.busy": "2025-03-15T21:44:06.319783Z",
          "iopub.status.idle": "2025-03-15T21:44:06.330984Z",
          "shell.execute_reply": "2025-03-15T21:44:06.330146Z",
          "shell.execute_reply.started": "2025-03-15T21:44:06.320006Z"
        },
        "id": "tTzOs65JydcS",
        "outputId": "e07177c9-2f2b-432a-8a97-9e507df166bf",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# test your implementation\n",
        "test_linear = nn.Linear(128, 128)\n",
        "test_linear.weight.data[...] = torch.eye(128)\n",
        "test_adapter = LoRALayer(test_linear, rank=8)\n",
        "\n",
        "assert torch.allclose(test_adapter(torch.ones(1, 1, 128)), test_linear.bias + 1), \"please check your forward pass\"\n",
        "\n",
        "test_adapter.adapter_A.data[...] = torch.linspace(0.1, -0.5, 128 * 8).view(128, 8)\n",
        "test_adapter.adapter_B.data[...] = torch.linspace(0.5, -0.1, 128 * 8).view(8, 128)\n",
        "test_linear.bias.data[...] = torch.linspace(1., -1., 128)\n",
        "\n",
        "dummy_loss = F.mse_loss(test_adapter(torch.ones(1, 128) / 128).squeeze(), torch.linspace(-1, 1, 128))\n",
        "assert torch.allclose(dummy_loss, torch.tensor(1.3711389), rtol=0, atol=1e-4)\n",
        "dummy_loss.backward()\n",
        "assert all(w.grad is not None for w in [test_adapter.adapter_A, test_adapter.adapter_B]), \"some adapter weights have no grad\"\n",
        "assert torch.allclose(test_adapter.adapter_A.grad.sum(), torch.tensor(-0.60158), rtol=0, atol=1e-4), \"bad grad w.r.t. A\"\n",
        "assert torch.allclose(test_adapter.adapter_B.grad.sum(), torch.tensor(0.9931), rtol=0, atol=1e-4), \"bad grad w.r.t. B\"\n",
        "# note: bad grad means that your code is different from LoRA paper OR that your code is not autograd-friendly (e.g. no_grad)\n",
        "del dummy_loss, test_linear, test_adapter\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tajVTsvLulB6"
      },
      "source": [
        "### Apply LoRA to the model\n",
        "\n",
        "The code below applies LoRA adapters on top of Q/K/V linear layers in Llama attention. You may also choose to modify other layers:\n",
        "* self_attn.o_proj - attention output projection\n",
        "* mlp.up_proj, mlp.gate_proj, mlp.down_proj - transformer feedforward layers\n",
        "* lm_head - output LM head\n",
        "\n",
        "__Note:__ please scroll down for the homework task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T02:14:02.287591Z",
          "iopub.status.busy": "2025-03-16T02:14:02.287304Z",
          "iopub.status.idle": "2025-03-16T02:14:02.322868Z",
          "shell.execute_reply": "2025-03-16T02:14:02.322276Z",
          "shell.execute_reply.started": "2025-03-16T02:14:02.287565Z"
        },
        "id": "davyUVEwulB6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "lora_rank = 8\n",
        "\n",
        "for name, module in model.model.layers.named_modules():\n",
        "    if 'LlamaDecoderLayer' in repr(type(module)):\n",
        "        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank).to(device)\n",
        "        module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank).to(device)\n",
        "        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(device)\n",
        "\n",
        "assert sum(isinstance(module, LoRALayer) for module in model.modules()) == 96  # for Llama-7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-15T21:44:42.162967Z",
          "iopub.status.busy": "2025-03-15T21:44:42.162673Z",
          "iopub.status.idle": "2025-03-15T21:44:42.8499Z",
          "shell.execute_reply": "2025-03-15T21:44:42.848889Z",
          "shell.execute_reply.started": "2025-03-15T21:44:42.162945Z"
        },
        "id": "AWzfvc0EulB6",
        "outputId": "b432afe7-08b9-4cb2-c6ac-01f85352a689",
        "tags": [],
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grad check successful, well done!\n"
          ]
        }
      ],
      "source": [
        "batch = tokenizer(\"This model wants to share its greatest secret:\", return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "# test a single training step, make sure we get meaningful gradients\n",
        "with torch.amp.autocast(device_type = 'cuda', dtype=torch.float32):\n",
        "    out = model.forward(**batch)\n",
        "    (out.logits.norm() / 100).backward()\n",
        "\n",
        "for i, module in enumerate(model.modules()):\n",
        "    if isinstance(module, LoRALayer):\n",
        "        assert module.adapter_B.grad is not None\n",
        "        assert module.adapter_B.grad.norm().item() > 0\n",
        "\n",
        "model.zero_grad(set_to_none=True)\n",
        "print(\"Grad check successful, well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjIJ1vkUulB7"
      },
      "source": [
        "### (example) How to train your model\n",
        "\n",
        "The example below shows how to train the LoRA adapters on a dummy dataset. You will need to run a _similar_ training task later.\n",
        "\n",
        "__Note:__ please scroll down for the homework task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "93ec6c07bea146ecbf5cfa9c91214afe",
            "eb473a48f4c443e4b8fce455d18ceaed",
            "8d74a2a77ef3474d94fbfdf682942475",
            "471b953c29794dad84adadaca18a8306",
            "8fc6f05cd3ce4434b7de1fd9aedeaca8",
            "93309656277b4f7c92dcdae32c9deb0f",
            "55ea330dd33f4b3f9c7249f6b2a72295",
            "08d066dca79e4922a9f2baa85705d83d",
            "2574861521a74595ab8c729209425177",
            "5244d74cf105401fb9d83eafcfefcfd7",
            "5d0f4d57848c4c71aab57d260501e43e"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T02:11:47.55534Z",
          "iopub.status.busy": "2025-03-16T02:11:47.554998Z",
          "iopub.status.idle": "2025-03-16T02:11:47.594339Z",
          "shell.execute_reply": "2025-03-16T02:11:47.593078Z",
          "shell.execute_reply.started": "2025-03-16T02:11:47.555319Z"
        },
        "id": "NvmLOj2oxZkK",
        "outputId": "b9097272-4a9e-4674-abb7-8c69c33b9b7f",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 1/1 [00:00<00:00, 114.36 examples/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask'],\n",
              "    num_rows: 1\n",
              "})"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "# Define prompt and completion\n",
        "texts = [\"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"]\n",
        "\n",
        "# Tokenizing the data\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=30)\n",
        "\n",
        "# Create a dataset\n",
        "dataset = Dataset.from_dict({\"text\": texts})\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Drop the original text column (optional)\n",
        "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T02:21:28.986757Z",
          "iopub.status.busy": "2025-03-16T02:21:28.986382Z",
          "iopub.status.idle": "2025-03-16T02:23:44.609219Z",
          "shell.execute_reply": "2025-03-16T02:23:44.607883Z",
          "shell.execute_reply.started": "2025-03-16T02:21:28.986727Z"
        },
        "id": "r9mIpntHulB8",
        "outputId": "1d838126-eddc-4543-dc78-ba8144b37b76",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='51' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 16:01, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.939100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.939100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.939100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.939100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.928600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.917400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.892100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.854800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.809000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.760600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.713800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.660100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.604200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.544200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.460300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.372600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.272600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.190200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.135700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.081400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.022100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.959100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.891700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.822800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.752600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.681600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.610500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.538300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.467500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.396500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.326000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.253100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.182100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.107400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.029600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.946700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.858500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.795200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.729000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.650300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.561000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.476200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.406100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.336200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.279800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.249500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.231900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.219400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.209100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'active_adapters' where it is not associated with a value",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[104]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      4\u001b[39m trainer = transformers.Trainer(\n\u001b[32m      5\u001b[39m     model=model, train_dataset=tokenized_dataset,\n\u001b[32m      6\u001b[39m     args=transformers.TrainingArguments(\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     12\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# NOTE: this is just an example! you do not have to wait for this progressbar to finish :)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mm:\\Classes\\AI Seminar\\aiseminar\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mm:\\Classes\\AI Seminar\\aiseminar\\Lib\\site-packages\\transformers\\trainer.py:2620\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2618\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m + steps_skipped) / steps_in_epoch\n\u001b[32m   2619\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2620\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\n\u001b[32m   2622\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2623\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2624\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mm:\\Classes\\AI Seminar\\aiseminar\\Lib\\site-packages\\transformers\\trainer.py:3100\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[39m\n\u001b[32m   3097\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n\u001b[32m   3099\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3100\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3101\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mm:\\Classes\\AI Seminar\\aiseminar\\Lib\\site-packages\\transformers\\trainer.py:3197\u001b[39m, in \u001b[36mTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial)\u001b[39m\n\u001b[32m   3195\u001b[39m run_dir = \u001b[38;5;28mself\u001b[39m._get_output_dir(trial=trial)\n\u001b[32m   3196\u001b[39m output_dir = os.path.join(run_dir, checkpoint_folder)\n\u001b[32m-> \u001b[39m\u001b[32m3197\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy \u001b[38;5;129;01min\u001b[39;00m [SaveStrategy.STEPS, SaveStrategy.EPOCH] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.best_global_step:\n\u001b[32m   3200\u001b[39m     best_checkpoint_folder = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPREFIX_CHECKPOINT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state.best_global_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mm:\\Classes\\AI Seminar\\aiseminar\\Lib\\site-packages\\transformers\\trainer.py:3884\u001b[39m, in \u001b[36mTrainer.save_model\u001b[39m\u001b[34m(self, output_dir, _internal_call)\u001b[39m\n\u001b[32m   3881\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_wrapped.save_checkpoint(output_dir)\n\u001b[32m   3883\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3884\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3886\u001b[39m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[32m   3887\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.push_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mm:\\Classes\\AI Seminar\\aiseminar\\Lib\\site-packages\\transformers\\trainer.py:3988\u001b[39m, in \u001b[36mTrainer._save\u001b[39m\u001b[34m(self, output_dir, state_dict)\u001b[39m\n\u001b[32m   3986\u001b[39m             torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n\u001b[32m   3987\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3988\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3989\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_safetensors\u001b[49m\n\u001b[32m   3990\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3992\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.processing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3993\u001b[39m     \u001b[38;5;28mself\u001b[39m.processing_class.save_pretrained(output_dir)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mm:\\Classes\\AI Seminar\\aiseminar\\Lib\\site-packages\\transformers\\modeling_utils.py:3371\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   3367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _hf_peft_config_loaded:\n\u001b[32m   3368\u001b[39m     logger.info(\n\u001b[32m   3369\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDetected adapters on the model, saving the model in the PEFT format, only adapter weights will be saved.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3370\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3371\u001b[39m     state_dict = \u001b[43mmodel_to_save\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_adapter_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m save_peft_format:\n\u001b[32m   3374\u001b[39m         logger.info(\n\u001b[32m   3375\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTo match the expected format of the PEFT library, all keys of the state dict of adapters will be pre-pended with `base_model.model`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3376\u001b[39m         )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mm:\\Classes\\AI Seminar\\aiseminar\\Lib\\site-packages\\transformers\\integrations\\peft.py:473\u001b[39m, in \u001b[36mPeftAdapterMixin.get_adapter_state_dict\u001b[39m\u001b[34m(self, adapter_name, state_dict)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_peft_model_state_dict\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     adapter_name = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactive_adapters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    475\u001b[39m adapter_state_dict = get_peft_model_state_dict(\u001b[38;5;28mself\u001b[39m, state_dict=state_dict, adapter_name=adapter_name)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m adapter_state_dict\n",
            "\u001b[36mFile \u001b[39m\u001b[32mm:\\Classes\\AI Seminar\\aiseminar\\Lib\\site-packages\\transformers\\integrations\\peft.py:437\u001b[39m, in \u001b[36mPeftAdapterMixin.active_adapters\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    434\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    436\u001b[39m \u001b[38;5;66;03m# For previous PEFT versions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mactive_adapters\u001b[49m, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    438\u001b[39m     active_adapters = [active_adapters]\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m active_adapters\n",
            "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'active_adapters' where it is not associated with a value"
          ]
        }
      ],
      "source": [
        "model._hf_peft_config_loaded = True  # silence a warning from HF trainer for hand-made LoRA\n",
        "\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model, train_dataset=tokenized_dataset,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1, gradient_accumulation_steps=4,\n",
        "        # note: if you want larger batch size, increase gradient_accumulation_steps\n",
        "        warmup_steps=250, max_steps=50, learning_rate=2e-4, fp16=True,\n",
        "        logging_steps=1, output_dir='outputs', report_to=\"none\"), # Unse repotr_to=None for Wandb\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# NOTE: this is just an example! you do not have to wait for this progressbar to finish :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HofX-vgQsBOE"
      },
      "source": [
        "You'll gett an error after training, but you still can do inference with this model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T02:23:59.162256Z",
          "iopub.status.busy": "2025-03-16T02:23:59.16195Z",
          "iopub.status.idle": "2025-03-16T02:23:59.172448Z",
          "shell.execute_reply": "2025-03-16T02:23:59.171589Z",
          "shell.execute_reply.started": "2025-03-16T02:23:59.162233Z"
        },
        "id": "2A0lAOtCxZkK",
        "outputId": "69a0e512-f86e-4d25-a1a6-6214b1e9fb6b",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): LoRALayer(\n",
              "            (module): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          )\n",
              "          (k_proj): LoRALayer(\n",
              "            (module): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          )\n",
              "          (v_proj): LoRALayer(\n",
              "            (module): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          )\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((4096,), eps=1e-06)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = trainer.model\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T02:24:06.288473Z",
          "iopub.status.busy": "2025-03-16T02:24:06.288143Z",
          "iopub.status.idle": "2025-03-16T02:24:08.262599Z",
          "shell.execute_reply": "2025-03-16T02:24:08.261756Z",
          "shell.execute_reply.started": "2025-03-16T02:24:06.288443Z"
        },
        "id": "IeXF3VwZxZkK",
        "outputId": "3a0defb8-4d00-4413-9abc-4d1eaf1bf4be",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\\nThe quick brown fox']\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer(['A quick brown fox'],\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=23, eos_token_id=3\n",
        "    )\n",
        "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "notebook58e9291559",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "aiseminar",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08d066dca79e4922a9f2baa85705d83d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09b07105e4f54d2bb5e6cc1c1f1a9c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2574861521a74595ab8c729209425177": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25e1f3d72230485c9b84cae4f685a69a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bd4b6acd8004c1e98c064708108938e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "471b953c29794dad84adadaca18a8306": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5244d74cf105401fb9d83eafcfefcfd7",
            "placeholder": "​",
            "style": "IPY_MODEL_5d0f4d57848c4c71aab57d260501e43e",
            "value": " 1/1 [00:00&lt;00:00, 12.81 examples/s]"
          }
        },
        "5244d74cf105401fb9d83eafcfefcfd7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "530d66e4732b4d5486165654415bd2dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55ea330dd33f4b3f9c7249f6b2a72295": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d0f4d57848c4c71aab57d260501e43e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6aad5b046def4a7db1048434e874b5d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aaa22971b3e416eae8394ef3b3b3f0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dba48e929a2e43ec8e4bb3d4b32475ca",
            "placeholder": "​",
            "style": "IPY_MODEL_530d66e4732b4d5486165654415bd2dc",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8d74a2a77ef3474d94fbfdf682942475": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08d066dca79e4922a9f2baa85705d83d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2574861521a74595ab8c729209425177",
            "value": 1
          }
        },
        "8fc6f05cd3ce4434b7de1fd9aedeaca8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "923869a5864c4d3d80fb76c99fff24e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93309656277b4f7c92dcdae32c9deb0f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93ec6c07bea146ecbf5cfa9c91214afe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb473a48f4c443e4b8fce455d18ceaed",
              "IPY_MODEL_8d74a2a77ef3474d94fbfdf682942475",
              "IPY_MODEL_471b953c29794dad84adadaca18a8306"
            ],
            "layout": "IPY_MODEL_8fc6f05cd3ce4434b7de1fd9aedeaca8"
          }
        },
        "b87c54c3bed847ab933eae8175359169": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8aaa22971b3e416eae8394ef3b3b3f0f",
              "IPY_MODEL_e9c4ba0d262c4b76baa00532e209b92f",
              "IPY_MODEL_e6f9064e6ec545debe2e90163f4c712c"
            ],
            "layout": "IPY_MODEL_6aad5b046def4a7db1048434e874b5d5"
          }
        },
        "dba48e929a2e43ec8e4bb3d4b32475ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6f9064e6ec545debe2e90163f4c712c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_923869a5864c4d3d80fb76c99fff24e2",
            "placeholder": "​",
            "style": "IPY_MODEL_25e1f3d72230485c9b84cae4f685a69a",
            "value": " 33/33 [01:49&lt;00:00,  3.12s/it]"
          }
        },
        "e9c4ba0d262c4b76baa00532e209b92f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bd4b6acd8004c1e98c064708108938e",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09b07105e4f54d2bb5e6cc1c1f1a9c8e",
            "value": 33
          }
        },
        "eb473a48f4c443e4b8fce455d18ceaed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93309656277b4f7c92dcdae32c9deb0f",
            "placeholder": "​",
            "style": "IPY_MODEL_55ea330dd33f4b3f9c7249f6b2a72295",
            "value": "Map: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
